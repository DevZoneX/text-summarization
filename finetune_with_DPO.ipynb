{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 11 14:32:38 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.04             Driver Version: 570.124.04     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A4000               Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 41%   31C    P8             10W /  140W |     152MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A           33544      G   /usr/libexec/Xorg                        94MiB |\n",
      "|    0   N/A  N/A           33650      G   /usr/bin/gnome-shell                     19MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/zakaria.abboud/.cache/huggingface\n",
      "New cache directory: /Data/zakaria.abboud/.cache/huggingface\n",
      "Path from root: /Data/zakaria.abboud/.cache/huggingface\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cache_dir = os.getenv(\"HF_HOME\", os.path.expanduser(\"~/.cache/huggingface\"))\n",
    "print(cache_dir)\n",
    "os.chdir(\"/\")\n",
    "new_cache_dir = \"/Data/zakaria.abboud/.cache/huggingface\"\n",
    "if not os.path.exists(new_cache_dir):\n",
    "    os.makedirs(new_cache_dir, exist_ok=True)\n",
    "\n",
    "# Set the new cache directory\n",
    "os.environ[\"HF_HOME\"] = new_cache_dir\n",
    "print(f\"New cache directory: {new_cache_dir}\")\n",
    "print(f\"Path from root: {os.path.abspath(new_cache_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/zakaria.abboud/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-11 14:33:03.870003: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-11 14:33:04.127791: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-11 14:33:05.109022: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-11 14:33:08.211966: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model to fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13271040 || all params: 328390528 || trainable%: 4.041237145548851\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules = [\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = PeftModel(model, peft_config=config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = json.load(open(\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/data_2k_tokens/train.json\"))\n",
    "test_dataset = json.load(open(\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/data_2k_tokens/test.json\"))\n",
    "\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "test_dataset = Dataset.from_list(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "\n",
    "    prompt = f\"<human>: Summarize the following text :\\n\\n {data_point['content']}\\n\\nYou should respond in French, with 5 sentences maximum.\\n\\n<assistant>:\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)+tokenizer.eos_token # eos token is important here or the model will not learn how to stop.\n",
    "    tokenized_full_prompt = tokenizer(full_prompt, return_tensors='pt')\n",
    "    labels = tokenized_full_prompt.input_ids.clone()\n",
    "\n",
    "    prompt = full_prompt[:full_prompt.find(\"<assistant>\")] + \"<assistant>:\"\n",
    "    \n",
    "    assistant_idx = tokenizer.encode(prompt, return_tensors='pt').shape[1]\n",
    "    labels[:, :assistant_idx] = -100\n",
    "\n",
    "    return {\n",
    "        'input_ids': tokenized_full_prompt.input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': tokenized_full_prompt.attention_mask.flatten(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data for the DPO fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/4589 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4589/4589 [00:00<00:00, 15391.52 examples/s]\n",
      "Map: 100%|██████████| 1967/1967 [00:00<00:00, 16166.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data_dpo(data_point):\n",
    "\n",
    "    system = \"You are a helpfull assistant, and you are asked to summarize the following text in French. You should respond with 5 sentences maximum:\\n\\n\"\n",
    "\n",
    "    return {\n",
    "            \"chosen\": [\n",
    "                {'role': 'user', 'content': system +\"\\n\"+ data_point[\"content\"]},\n",
    "                {'role': 'assistant', 'content': data_point[\"chosen\"]}\n",
    "            ],\n",
    "            \"rejected\": [\n",
    "                {'role': 'user', 'content': system +\"\\n\"+ data_point[\"content\"]},\n",
    "                {'role': 'assistant', 'content': data_point[\"rejected\"]}\n",
    "            ],\n",
    "        }\n",
    "\n",
    "data_dpo = train_dataset.shuffle(seed=42).map(preprocess_data_dpo)\n",
    "test_dpo = test_dataset.shuffle(seed=42).map(preprocess_data_dpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['title', 'content', 'chosen', 'rejected'],\n",
      "    num_rows: 4589\n",
      "})\n",
      "[{'content': \"You are a helpfull assistant, and you are asked to summarize the following text in French. You should respond with 5 sentences maximum:\\n\\n\\nCórrego do Bom Jesus est une municipalité brésilienne de l'État du Minas Gerais et la microrégion de Pouso Alegre.\\n\\n\\n== Notes et références ==\\n\\n Portail du Minas Gerais\", 'role': 'user'}, {'content': \"Córrego do Bom Jesus est une municipalité située dans l'État du Minas Gerais au Brésil. Elle fait partie de la microrégion de Pouso Alegre. Pour plus d'informations, consultez le Portail du Minas Gerais. Cette municipalité est donc un territoire administratif et géographique bien définis dans cette région brésilienne. <sentence-break>\", 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print(data_dpo)\n",
    "print(data_dpo[0][\"chosen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: You are a helpfull assistant, and you are asked to summarize the following text in French. You should respond with 5 sentences maximum:\n",
      "\n",
      "\n",
      "Córrego do Bom Jesus est une municipalité brésilienne de l'État du Minas Gerais et la microrégion de Pouso Alegre.\n",
      "\n",
      "\n",
      "== Notes et références ==\n",
      "\n",
      " Portail du Minas Gerais\n",
      "<assistant>: Ce terme d'île ou de île en général désigne un petit bateau qui a été déplacé pour y survivre. Dans le cas de l'île Corrêa, elle était principalement utilisée par les peuples indigènes au fil des siècles. L'île est maintenant considérée comme une propriété privée. Elle est située dans l'est du Minas Gerais, à environ 180 km au nord-est de Lisbonne. \n",
      "\n",
      "<assistant>: Córrego do Bom Jesus est une municipalité brésilienne située dans l<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}<human>: {{ message['content'] }}\\n{% elif message['role'] == 'assistant' %}<assistant>: {{ message['content'] }}{{ eos_token }}{% endif %}{% endfor %}{% if add_generation_prompt %}<assistant>: {% endif %}\"\n",
    "\n",
    "messages_example =  data_dpo[0][\"rejected\"]\n",
    "\n",
    "print(tokenizer.apply_chat_template(messages_example, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_348246/905332308.py:21: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `DPOTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = DPOTrainer(\n",
      "Extracting prompt in train dataset: 100%|██████████| 4589/4589 [00:00<00:00, 9136.60 examples/s]\n",
      "Applying chat template to train dataset: 100%|██████████| 4589/4589 [00:00<00:00, 7383.34 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 4589/4589 [00:05<00:00, 768.07 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 17:28, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.014300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.003400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.007930982336401939, metrics={'train_runtime': 1058.6599, 'train_samples_per_second': 3.023, 'train_steps_per_second': 0.094, 'total_flos': 0.0, 'train_loss': 0.007930982336401939, 'epoch': 0.6973196774896492})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR = \"/Data/zakaria.abboud/dpo_output_2k_tokens\"\n",
    "\n",
    "training_args = DPOConfig(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=32,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-5,\n",
    "    bf16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=20,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_steps=100,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    report_to=\"tensorboard\",\n",
    "    beta=0.1,\n",
    "    label_names=[\"chosen\", \"rejected\"],\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=data_dpo,\n",
    "    tokenizer=tokenizer,\n",
    "    # Data collator is not needed for DPOTrainer as it internally manages it\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 896)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "              (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "              (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "              (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/zakaria.abboud/.local/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Save the model weights\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/models/2k_tokens_finetuned\")\n",
    "tokenizer.save_pretrained(\"users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/models/2k_tokens_finetuned\")\n",
    "# model.config.save_pretrained(\"users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/models/2k_tokens\")\n",
    "print(\"Model saved successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Computing the new summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/models/2k_tokens_finetuned\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/models/2k_tokens_finetuned\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_document(document):\n",
    "    # Tokenize the input document\n",
    "    prompt = f\"<human>: Summarize the following text :\\n\\n{document}\\n\\nYou should respond in French, with 5 sentences maximum.\\n\\n<assistant>:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    # Decode the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    assistant_index = summary.find(\"<assistant>:\")\n",
    "    if assistant_index != -1:\n",
    "        summary = summary[assistant_index + len(\"<assistant>:\"):].strip()\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = json.load(open(\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/summaries/2k_wikipedia_articles.json\"))\n",
    "# summaries = json.load(open(\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/summaries/2k_finetuned_dpo_summarized_wikipedia_articles.json\"))\n",
    "\n",
    "\n",
    "# articles = articles[len(summaries):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model before computing the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La rata est un ragoût grossier consommé principalement par les agriculteurs et les soldats, ce qui a contribué à son nom de \"rôle\" en français. Il s'agit d'une variété de viandes (par exemple, porc, bœuf, mouton ou harteckel) accompagnée de légumes comme la pomme de terre, les carottes, les haricots, les oignons et autres commerces. \n",
      "\n",
      "Les citoyens ont tendance à désigner une personne qui parle beaucoup comme \"moulin à rata\".\n"
     ]
    }
   ],
   "source": [
    "test_text = articles[1][\"content\"]\n",
    "print(summarize_document(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing articles:   0%|          | 0/6498 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing articles:  33%|███▎      | 2129/6498 [1:03:29<1:34:06,  1.29s/it]"
     ]
    }
   ],
   "source": [
    "# File to save the summarized articles\n",
    "# output_file = '/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/summaries/summarized_wikipedia_articles.json'\n",
    "output_file = '/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/summaries/2k_finetuned_dpo_summarized_wikipedia_articles.json'\n",
    "\n",
    "# Function to save results progressively\n",
    "def save_progress(summarized_articles, output_file):\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            existing_data = json.load(f)\n",
    "    else:\n",
    "        existing_data = []\n",
    "\n",
    "    existing_data.extend(summarized_articles)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Summarize articles in parallel\n",
    "summarized_articles = []\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust max_workers based on your system\n",
    "    futures = {executor.submit(summarize_document, article['content']): article for article in articles}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Summarizing articles\"):\n",
    "        result = future.result()\n",
    "        summarized_articles.append({\n",
    "            'title': futures[future]['title'],\n",
    "            'content': futures[future]['content'],\n",
    "            'summary': result\n",
    "        })\n",
    "\n",
    "        # Save progress every 10 summaries\n",
    "        if len(summarized_articles) % 10 == 0:\n",
    "            save_progress(summarized_articles, output_file)\n",
    "            summarized_articles = []  # Clear the list after saving\n",
    "\n",
    "# Save any remaining summaries\n",
    "if summarized_articles:\n",
    "    save_progress(summarized_articles, output_file)\n",
    "\n",
    "print(f\"Summarized articles saved to '{output_file}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "my_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
