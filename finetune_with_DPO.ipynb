{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/zakaria.abboud/.cache/huggingface\n",
      "New cache directory: /Data/zakaria.abboud/.cache/huggingface\n",
      "Path from root: /Data/zakaria.abboud/.cache/huggingface\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cache_dir = os.getenv(\"HF_HOME\", os.path.expanduser(\"~/.cache/huggingface\"))\n",
    "print(cache_dir)\n",
    "os.chdir(\"/\")\n",
    "new_cache_dir = \"/Data/zakaria.abboud/.cache/huggingface\"\n",
    "if not os.path.exists(new_cache_dir):\n",
    "    os.makedirs(new_cache_dir, exist_ok=True)\n",
    "\n",
    "# Set the new cache directory\n",
    "os.environ[\"HF_HOME\"] = new_cache_dir\n",
    "print(f\"New cache directory: {new_cache_dir}\")\n",
    "print(f\"Path from root: {os.path.abspath(new_cache_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model to fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13271040 || all params: 328390528 || trainable%: 4.041237145548851\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules = [\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = PeftModel(model, peft_config=config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_size = \"2k\" # or 8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = json.load(open(f\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/data_{token_size}_tokens/train.json\"))\n",
    "test_dataset = json.load(open(f\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/data_{token_size}_tokens/test.json\"))\n",
    "\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "test_dataset = Dataset.from_list(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data for the DPO fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/4589 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4589/4589 [00:00<00:00, 15391.52 examples/s]\n",
      "Map: 100%|██████████| 1967/1967 [00:00<00:00, 16166.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data_dpo(data_point):\n",
    "\n",
    "    system = \"You are a helpfull assistant, and you are asked to summarize the following text in French. You should respond with 5 sentences maximum:\\n\\n\"\n",
    "\n",
    "    return {\n",
    "            \"chosen\": [\n",
    "                {'role': 'user', 'content': system +\"\\n\"+ data_point[\"content\"]},\n",
    "                {'role': 'assistant', 'content': data_point[\"chosen\"]}\n",
    "            ],\n",
    "            \"rejected\": [\n",
    "                {'role': 'user', 'content': system +\"\\n\"+ data_point[\"content\"]},\n",
    "                {'role': 'assistant', 'content': data_point[\"rejected\"]}\n",
    "            ],\n",
    "        }\n",
    "\n",
    "data_dpo = train_dataset.shuffle(seed=42).map(preprocess_data_dpo)\n",
    "test_dpo = test_dataset.shuffle(seed=42).map(preprocess_data_dpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['title', 'content', 'chosen', 'rejected'],\n",
      "    num_rows: 4589\n",
      "})\n",
      "[{'content': \"You are a helpfull assistant, and you are asked to summarize the following text in French. You should respond with 5 sentences maximum:\\n\\n\\nCórrego do Bom Jesus est une municipalité brésilienne de l'État du Minas Gerais et la microrégion de Pouso Alegre.\\n\\n\\n== Notes et références ==\\n\\n Portail du Minas Gerais\", 'role': 'user'}, {'content': \"Córrego do Bom Jesus est une municipalité située dans l'État du Minas Gerais au Brésil. Elle fait partie de la microrégion de Pouso Alegre. Pour plus d'informations, consultez le Portail du Minas Gerais. Cette municipalité est donc un territoire administratif et géographique bien définis dans cette région brésilienne. <sentence-break>\", 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print(data_dpo)\n",
    "print(data_dpo[0][\"chosen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: You are a helpfull assistant, and you are asked to summarize the following text in French. You should respond with 5 sentences maximum:\n",
      "\n",
      "\n",
      "Córrego do Bom Jesus est une municipalité brésilienne de l'État du Minas Gerais et la microrégion de Pouso Alegre.\n",
      "\n",
      "\n",
      "== Notes et références ==\n",
      "\n",
      " Portail du Minas Gerais\n",
      "<assistant>: Ce terme d'île ou de île en général désigne un petit bateau qui a été déplacé pour y survivre. Dans le cas de l'île Corrêa, elle était principalement utilisée par les peuples indigènes au fil des siècles. L'île est maintenant considérée comme une propriété privée. Elle est située dans l'est du Minas Gerais, à environ 180 km au nord-est de Lisbonne. \n",
      "\n",
      "<assistant>: Córrego do Bom Jesus est une municipalité brésilienne située dans l<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}<human>: {{ message['content'] }}\\n{% elif message['role'] == 'assistant' %}<assistant>: {{ message['content'] }}{{ eos_token }}{% endif %}{% endfor %}{% if add_generation_prompt %}<assistant>: {% endif %}\"\n",
    "\n",
    "messages_example =  data_dpo[0][\"rejected\"]\n",
    "\n",
    "print(tokenizer.apply_chat_template(messages_example, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_348246/905332308.py:21: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `DPOTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = DPOTrainer(\n",
      "Extracting prompt in train dataset: 100%|██████████| 4589/4589 [00:00<00:00, 9136.60 examples/s]\n",
      "Applying chat template to train dataset: 100%|██████████| 4589/4589 [00:00<00:00, 7383.34 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 4589/4589 [00:05<00:00, 768.07 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 17:28, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.014300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.003400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.007930982336401939, metrics={'train_runtime': 1058.6599, 'train_samples_per_second': 3.023, 'train_steps_per_second': 0.094, 'total_flos': 0.0, 'train_loss': 0.007930982336401939, 'epoch': 0.6973196774896492})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR = \"/Data/zakaria.abboud/dpo_output_2k_tokens\"\n",
    "\n",
    "training_args = DPOConfig(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=32,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-5,\n",
    "    bf16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=20,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_steps=100,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    report_to=\"tensorboard\",\n",
    "    beta=0.1,\n",
    "    label_names=[\"chosen\", \"rejected\"],\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=data_dpo,\n",
    "    tokenizer=tokenizer,\n",
    "    # Data collator is not needed for DPOTrainer as it internally manages it\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 896)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "              (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "              (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "              (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/zakaria.abboud/.local/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Save the model weights\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/models/2k_tokens_finetuned\")\n",
    "tokenizer.save_pretrained(\"users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/models/2k_tokens_finetuned\")\n",
    "# model.config.save_pretrained(\"users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/models/2k_tokens\")\n",
    "print(\"Model saved successfully\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "my_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
