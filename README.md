### **README for Fine-Tuning a Small Language Model (SLM) for Summarization**  

---

## **Project Overview**  
This project focuses on fine-tuning a **Small Language Model (SLM)** (e.g., **mT5-Base, Qwen2.5-0.5B, Atlas-Chat-2B**) for **summarization** of **non-English text**.  
We implement **LoRA (Low-Rank Adaptation)** and **custom quantization** for efficient training on **Google Colabâ€™s free-tier GPU**.  
Evaluation includes **ROUGE, BERTScore, and LLM-as-a-Judge**.  

---

## **Project Structure**  

ğŸ“‚ **Fine-Tuned-SLM/** *(Root Directory)*  
 â”œâ”€â”€ `requirements.txt` â€“ Dependencies  
 â”œâ”€â”€ `data.ipynb` â€“ Jupyter Notebook for preparing dataset
 â””â”€â”€ `finetune.ipynb` â€“ Jupyter Notebook for training, inference, and evaluation
 â”‚  
 â”œâ”€â”€ ğŸ“‚ **data/** *(Contains dataset - preprocessed files)*  
 â”‚   â”œâ”€â”€ `train.json` â€“ Training dataset  
 â”‚   â”œâ”€â”€ `test.json` â€“ Test dataset  
 â”‚   â””â”€â”€ `val.json` â€“ Validation dataset  
 â”‚  
 â”œâ”€â”€ ğŸ“‚ **results/** *(Contains model checkpoints and evaluation results)*  
 â”‚   â”œâ”€â”€ `fine_tuned_model/` â€“ Saved model and tokenizer  
 â”‚   â”œâ”€â”€ `evaluation_metrics.json` â€“ ROUGE & BERTScore results  
 â”‚   â””â”€â”€ `sample_summaries.txt` â€“ Example generated summaries  
 â”‚  
 â”œâ”€â”€ `README.md` *(This file)*  
 â”œâ”€â”€ `report.pdf` *(Project report)*
 â”œâ”€â”€ `run.sh` *(Bash script to automate training & inference)*

---

## **Installation & Setup**  
### **1ï¸âƒ£ Install Dependencies**  
Run the following command to install required libraries:  
```bash
pip install -r requirements.txt
```
Or install manually:  
```bash
pip install transformers datasets accelerate bitsandbytes peft rouge-score sacrebleu torch
```

## **Collaborators**
- [**Abboud Zakaria**]
- [**Brahim Touayouch**]