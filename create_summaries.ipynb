{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_model = None # DPO, SFT, or None\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "# Create a BitsAndBytesConfig for 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "if finetuned_model:\n",
    "    path = f\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/models/2k_{finetuned_model}_finetuned\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_document(document, model=model, tokenizer=tokenizer):\n",
    "    # Tokenize the input document\n",
    "    prompt = f\"<human>: Summarize the following text :\\n\\n{document}\\n\\nYou should respond with 5 sentences maximum.\\n\\n<assistant>:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    # Decode the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    assistant_index = summary.find(\"<assistant>:\")\n",
    "    if assistant_index != -1:\n",
    "        summary = summary[assistant_index + len(\"<assistant>:\"):].strip()\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the articles and summarizing them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the model (0.5B or 7B), we are generating good or bad summaries. In order to fine-tune the smaller model with the generated summaries by the bigger model. We needed to generate the summaries with the smaller model to compare them with the summaries generated by the bigger model, and also to use DPO in fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_size = \"2k\" # or 8k\n",
    "\n",
    "# Load the dataset\n",
    "articles = json.load(open(f\"articles/{token_size}_wikipedia_articles.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing articles: 100%|██████████| 2408/2408 [1:03:54<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized articles saved to '/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/summaries/2k_bad_summarized_wikipedia_articles.json'\n"
     ]
    }
   ],
   "source": [
    "# File to save the summarized articles\n",
    "output_file = f'summaries/{token_size}_bad_summarized_wikipedia_articles.json' # change the name of the file depending on the model used\n",
    "\n",
    "# Function to save results progressively\n",
    "def save_progress(summarized_articles, output_file):\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            existing_data = json.load(f)\n",
    "    else:\n",
    "        existing_data = []\n",
    "\n",
    "    existing_data.extend(summarized_articles)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Summarize articles in parallel\n",
    "summarized_articles = []\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust max_workers based on your system\n",
    "    futures = {executor.submit(summarize_document, article['content']): article for article in articles}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Summarizing articles\"):\n",
    "        result = future.result()\n",
    "        summarized_articles.append({\n",
    "            'title': futures[future]['title'],\n",
    "            'content': futures[future]['content'],\n",
    "            'summary': result\n",
    "        })\n",
    "\n",
    "        # Save progress every 10 summaries\n",
    "        if len(summarized_articles) % 10 == 0:\n",
    "            save_progress(summarized_articles, output_file)\n",
    "            summarized_articles = []  # Clear the list after saving\n",
    "\n",
    "# Save any remaining summaries\n",
    "if summarized_articles:\n",
    "    save_progress(summarized_articles, output_file)\n",
    "\n",
    "print(f\"Summarized articles saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping the bad and good summaries to prepare the data for DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries saved to 2k_summaries.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "good_summaries = json.load(open('summaries/2k_good_summarized_wikipedia_articles.json', 'r', encoding='utf-8'))\n",
    "bad_summaries = json.load(open('summaries/2k_bad_summarized_wikipedia_articles.json', 'r', encoding='utf-8'))\n",
    "\n",
    "all_summaries = []\n",
    "\n",
    "for good in good_summaries:\n",
    "    for bad in bad_summaries:\n",
    "        if good['title'] == bad['title']:\n",
    "            all_summaries.append({\n",
    "                'title': good['title'],\n",
    "                'content': good['content'],\n",
    "                'chosen': good['summary'],\n",
    "                'rejected': bad['summary']\n",
    "            })\n",
    "\n",
    "with open('/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/summaries/2k_summarized_wikipedia_articles.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_summaries, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Summaries saved to '2k_summarized_wikipedia_articles.json'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "\n",
    "train_data = all_summaries[:int(train_size * len(all_summaries))]\n",
    "test_data = all_summaries[int(train_size * len(all_summaries)):]\n",
    "\n",
    "with open(f'data_{token_size}_tokens/train.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(f'data_{token_size}_tokens/test.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_data, f, ensure_ascii=False, indent=4)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
