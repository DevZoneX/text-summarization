{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the environment and installing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 11 14:43:18 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.04             Driver Version: 570.124.04     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A4000               Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 65%   81C    P2            127W /  140W |   10143MiB /  16376MiB |     70%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A           33544      G   /usr/libexec/Xorg                        94MiB |\n",
      "|    0   N/A  N/A           33650      G   /usr/bin/gnome-shell                     19MiB |\n",
      "|    0   N/A  N/A          348246      C   /usr/bin/python                        9858MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets bitsandbytes peft torch sentencepiece -q\n",
    "%pip install rouge-score -q\n",
    "%pip install blobfile tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Data/zakaria.abboud/.cache/huggingface\n",
      "New cache directory: /Data/zakaria.abboud/.cache/huggingface\n",
      "Path from root: /Data/zakaria.abboud/.cache/huggingface\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "import os\n",
    "cache_dir = os.getenv(\"HF_HOME\", os.path.expanduser(\"~/.cache/huggingface\"))\n",
    "print(cache_dir)\n",
    "os.chdir(\"/\")\n",
    "new_cache_dir = \"/Data/zakaria.abboud/.cache/huggingface\"\n",
    "if not os.path.exists(new_cache_dir):\n",
    "    os.makedirs(new_cache_dir, exist_ok=True)\n",
    "\n",
    "# Set the new cache directory\n",
    "os.environ[\"HF_HOME\"] = new_cache_dir\n",
    "print(f\"New cache directory: {new_cache_dir}\")\n",
    "print(f\"Path from root: {os.path.abspath(new_cache_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the preparation, the dataset looks like this:\n",
    "```json\n",
    "{\n",
    "  \"title\": \"Document title...\",\n",
    "  \"content\": \"Original document text...\",\n",
    "  \"chosen\": \"Generated summary with the big model...\",\n",
    "  \"rejected\": \"Generated summary with the smaller model...\"\n",
    "}\n",
    "```\n",
    "And it's split into `train`, `validation`, and `test` sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files={\"train\": \"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/data_2k/\"+\"train.json\", \"val\": \"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/data_2k/\"+\"val.json\", \"test\": \"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/data_2k/\"+\"test.json\"})\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'école de médecine de Salerne, fondée au XIe siècle, combinaison de traditions grecques, romaines et orientales, a joué un rôle crucial dans la médecine médiévale en considérant la médecine comme une science basée sur le raisonnement. Elle a également influencé la médecine populaire avec des ouvrages comme le \"Regimen sanitatis\", qui ont inspiré des guides de santé populaires jusqu'à nos jours. Cette école a développé des méthodes d'enseignement innovantes, précurseurs de celles utilisées\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0][\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Custom Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load the model in 4-bit\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several models interesting to fine-tune for this task. We will use the `transformers` library to fine-tune a model from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"mT5-Base\": \"google/mt5-base\", # 580M params\n",
    "    \"Qwen2.5-0.5B\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "}\n",
    "\n",
    "model_name = models[\"Qwen2.5-0.5B\"]\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "# print(f\"Loaded model: {model_name}\\n\", model)\n",
    "# print(model.config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# print(f\"Loaded Tokenizer: \\n\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the tokenizer to encode the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1415766c84c422892f0d7eb24c60e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4548 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Add EOS token manually to both content and summary\n",
    "    full_prompt = f\"<human>: Résume {examples['content']}\\n<assistant>: {examples['summary']}\"\n",
    "\n",
    "    # Tokenize the full prompt\n",
    "    tokenized_full_prompt = tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Clone the tokenized input to create labels\n",
    "    labels = tokenized_full_prompt[\"input_ids\"].clone()\n",
    "\n",
    "    prompt = full_prompt[:full_prompt.find(\"<assistant>\")] + \"<assistant>:\"\n",
    "\n",
    "    end_prompt_index = tokenizer(prompt, return_tensors=\"pt\").input_ids.shape[1]\n",
    "\n",
    "    labels[:, :end_prompt_index] = -100\n",
    "\n",
    "    return {\n",
    "        'input_ids': tokenized_full_prompt[\"input_ids\"].flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': tokenized_full_prompt[\"attention_mask\"].flatten(),\n",
    "    }\n",
    "\n",
    "# Tokenize the dataset\n",
    "\n",
    "data = dataset[\"train\"].shuffle(seed=42).map(tokenize_function)\n",
    "    \n",
    "\n",
    "# data = dataset[\"train\"].shuffle(seed=42).map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Lora to fine-tune the model efficiently without having to adjust all the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,271,040 || all params: 507,303,808 || trainable%: 2.6160\n"
     ]
    }
   ],
   "source": [
    "# Extract target_modules\n",
    "target_modules = [\n",
    "    'gate_proj',\n",
    "    'up_proj',\n",
    "    'down_proj',\n",
    "]\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "generation_config.do_sample = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generation_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:13\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generation_config' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "article = \"\"\"Le Monument-Lefebvre est un lieu historique national du Canada situé à Memramcook, au Nouveau-Brunswick.\\n\\n\\n== Histoire ==\\nLe Monument-Lefebvre est inauguré les 16 et 17 juin 1897 et nommé ainsi en l'honneur du père Camille Lefebvre, fondateur du Collège Saint-Joseph, décédé deux ans auparavant.\\nIl est tout d'abord utilisé comme laboratoire par le Collège Saint-Joseph, tandis que son amphithéâtre permet d'organiser des concerts, des conférences et de nombreuses manifestations diverses. Le lieu et son fondateur ont joué un rôle déterminant dans la renaissance acadienne.\\nAprès la fermeture du Collège en 1972, le Monument-Lefebvre manque de disparaître mais est finalement rénové et déclaré lieu historique national en 1994.\\n\\n\\n== Architecture ==\\nLe Monument-Lefebvre est construit en grès rustiqué de couleur olive. Sa façade symétrique comprend des éléments néoromans. Il abrite un théâtre et des salle de classe.\\n\\n\\n== Notes et références ==\\n\\n\\n== Liens externes ==\\nRessources relatives à l'architecture : Édifices fédéraux patrimoniaux Lieux historiques nationaux Répertoire canadien des lieux patrimoniaux \\n(fr + en) Site officiel\\n\\n Portail de l’Acadie   Portail du Nouveau-Brunswick   Portail des lieux patrimoniaux du Canada\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "<human>: résume {article}\n",
    "<assistant>:\n",
    "\"\"\"\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=32,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=20,\n",
    "    output_dir=\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/results\",\n",
    "    max_steps=500,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 52:24, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.909900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.919300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.973800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.005200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.804800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.769600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.778600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.786800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.771200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.776100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.776600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.645700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.610800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.603200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.605500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.586900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.589900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.583100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.559700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.516500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.503700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.526200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.7453178424835205, metrics={'train_runtime': 3151.5771, 'train_samples_per_second': 5.077, 'train_steps_per_second': 0.159, 'total_flos': 4.016179539972557e+16, 'train_loss': 0.7453178424835205, 'epoch': 3.499560246262093})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<human>: Résume Le Monument-Lefebvre est un lieu historique national du Canada situé à Memramcook, au Nouveau-Brunswick.\n",
      "\n",
      "\n",
      "== Histoire ==\n",
      "Le Monument-Lefebvre est inauguré les 16 et 17 juin 1897 et nommé ainsi en l'honneur du père Camille Lefebvre, fondateur du Collège Saint-Joseph, décédé deux ans auparavant.\n",
      "Il est tout d'abord utilisé comme laboratoire par le Collège Saint-Joseph, tandis que son amphithéâtre permet d'organiser des concerts, des conférences et de nombreuses manifestations diverses. Le lieu et son fondateur ont joué un rôle déterminant dans la renaissance acadienne.\n",
      "Après la fermeture du Collège en 1972, le Monument-Lefebvre manque de disparaître mais est finalement rénové et déclaré lieu historique national en 1994.\n",
      "\n",
      "\n",
      "== Architecture ==\n",
      "Le Monument-Lefebvre est construit en grès rustiqué de couleur olive. Sa façade symétrique comprend des éléments néoromans. Il abrite un théâtre et des salle de classe.\n",
      "\n",
      "\n",
      "== Notes et références ==\n",
      "\n",
      "\n",
      "== Liens externes ==\n",
      "Ressources relatives à l'architecture : Édifices fédéraux patrimoniaux Lieux historiques nationaux Répertoire canadien des lieux patrimoniaux \n",
      "(fr + en) Site officiel\n",
      "\n",
      " Portail de l’Acadie   Portail du Nouveau-Brunswick   Portail des lieux patrimoniaux du Canada\n",
      "<assistant>:\n",
      "Human: Le Monument-Lefebvre est un monument historique national canadien situé à Memramcook, au Nouveau-Brunswick. Il a été inauguré en 1897 et nommé en hommage à Camille Lefebvre, fondateur du Collège Saint-Joseph. Utilisé comme laboratoire et lieu de concert, il a joué un rôle crucial dans la renaissance acadienne. Après sa fermeture en 1972, il a été rénové et classé monument historique national en 1994. Sa façade symétrique et ses éléments néoromans le caractérisent. <\\/p> <\\/div> <\\/div> <\\/div> <\\/div> <\\/div> <\\/div> <\\/div> <\\/div> <\\/div> <\\/div> <\\/div> <\\/div> <\\/div> <\\/\n",
      "CPU times: user 3.94 s, sys: 51.1 ms, total: 3.99 s\n",
      "Wall time: 4.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "article = \"\"\"Le Monument-Lefebvre est un lieu historique national du Canada situé à Memramcook, au Nouveau-Brunswick.\\n\\n\\n== Histoire ==\\nLe Monument-Lefebvre est inauguré les 16 et 17 juin 1897 et nommé ainsi en l'honneur du père Camille Lefebvre, fondateur du Collège Saint-Joseph, décédé deux ans auparavant.\\nIl est tout d'abord utilisé comme laboratoire par le Collège Saint-Joseph, tandis que son amphithéâtre permet d'organiser des concerts, des conférences et de nombreuses manifestations diverses. Le lieu et son fondateur ont joué un rôle déterminant dans la renaissance acadienne.\\nAprès la fermeture du Collège en 1972, le Monument-Lefebvre manque de disparaître mais est finalement rénové et déclaré lieu historique national en 1994.\\n\\n\\n== Architecture ==\\nLe Monument-Lefebvre est construit en grès rustiqué de couleur olive. Sa façade symétrique comprend des éléments néoromans. Il abrite un théâtre et des salle de classe.\\n\\n\\n== Notes et références ==\\n\\n\\n== Liens externes ==\\nRessources relatives à l'architecture : Édifices fédéraux patrimoniaux Lieux historiques nationaux Répertoire canadien des lieux patrimoniaux \\n(fr + en) Site officiel\\n\\n Portail de l’Acadie   Portail du Nouveau-Brunswick   Portail des lieux patrimoniaux du Canada\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "<human>: Résume {article}\n",
    "<assistant>:\n",
    "\"\"\"\n",
    "\n",
    "encoding = tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=2048, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Save the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 896)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "              (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "              (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "              (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "finetunedmodel = model.merge_and_unload()\n",
    "finetunedmodel.save_pretrained(\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/models/finetuned_2k_token\")\n",
    "finetunedmodel\n",
    "# load the model\n",
    "# model_2 = AutoModelForCausalLM.from_pretrained(\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the new summaries with the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_document(document):\n",
    "    # Tokenize the input document\n",
    "    prompt = f\"<human>: Summarize the following text :\\n\\n{document}\\n\\nYou should respond in French, with 5 sentences maximum.\\n\\n<assistant>:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    # Decode the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    assistant_index = summary.find(\"<assistant>:\")\n",
    "    if assistant_index != -1:\n",
    "        summary = summary[assistant_index + len(\"<assistant>:\"):].strip()\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6498, 460)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "# test\n",
    "articles = json.load(open('/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/summaries/2k_wikipedia_articles.json', 'r', encoding='utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing articles: 100%|██████████| 6498/6498 [4:46:55<00:00,  2.65s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized articles saved to '/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/summaries/2k_finetuned_summarized_wikipedia_articles.json'\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# File to save the summarized articles\n",
    "output_file = '/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/summaries/2k_finetuned_summarized_wikipedia_articles.json'\n",
    "\n",
    "# Function to save results progressively\n",
    "def save_progress(summarized_articles, output_file):\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            existing_data = json.load(f)\n",
    "    else:\n",
    "        existing_data = []\n",
    "\n",
    "    existing_data.extend(summarized_articles)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Summarize articles in parallel\n",
    "summarized_articles = []\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust max_workers based on your system\n",
    "    futures = {executor.submit(summarize_document, article['content']): article for article in articles}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Summarizing articles\"):\n",
    "        result = future.result()\n",
    "        summarized_articles.append({\n",
    "            'title': futures[future]['title'],\n",
    "            'content': futures[future]['content'],\n",
    "            'summary': result\n",
    "        })\n",
    "\n",
    "        # Save progress every 10 summaries\n",
    "        if len(summarized_articles) % 10 == 0:\n",
    "            save_progress(summarized_articles, output_file)\n",
    "            summarized_articles = []  # Clear the list after saving\n",
    "\n",
    "# Save any remaining summaries\n",
    "if summarized_articles:\n",
    "    save_progress(summarized_articles, output_file)\n",
    "\n",
    "print(f\"Summarized articles saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return result\n",
    "\n",
    "results = compute_metrics(trainer.predict(tokenized_datasets[\"test\"]))\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Bert Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
