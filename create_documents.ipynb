{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating documents from Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching 5000 random articles from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set language to French\n",
    "wikipedia.set_lang(\"fr\")\n",
    "\n",
    "# Function to fetch a single random article\n",
    "def fetch_random_article():\n",
    "    try:\n",
    "        title = wikipedia.random(pages=1)\n",
    "        page = wikipedia.page(title)\n",
    "        return {'title': page.title, 'content': page.content}\n",
    "    except (wikipedia.exceptions.DisambiguationError, wikipedia.exceptions.PageError):\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to fetch multiple random articles in parallel\n",
    "def fetch_random_articles(num_articles):\n",
    "    articles = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(fetch_random_article) for _ in range(num_articles)]\n",
    "        for future in tqdm(as_completed(futures), total=num_articles, desc=\"Fetching articles\"):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                articles.append(result)\n",
    "    return articles\n",
    "\n",
    "# Fetch 5000 random articles\n",
    "random_articles = fetch_random_articles(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposing the articles into parts of 8000/2000 tokens each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decompose the articles into parts of 8000/2000 tokens each, because the maximum number of tokens that can be processed by the model is 8000 tokens. \n",
    "And we could not train our fine-tuning model on the 8000 tokens because of the memory constraints. So we trained our model on 2000 tokens. But we still managed to train the model with DPO on 8000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decomposing articles: 100%|██████████| 4714/4714 [00:12<00:00, 378.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposed dataset saved to 'decomposed_wikipedia_articles.json'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "random_articles = json.load(open('/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/summaries/random_wikipedia_articles.json',\"r\"))\n",
    "\n",
    "# Model name\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to split content into chunks\n",
    "def split_content_into_chunks(content, max_tokens=7800):\n",
    "    # Tokenize the content\n",
    "    tokens = tokenizer(content, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0]\n",
    "\n",
    "    # Split tokens into chunks\n",
    "    chunks = []\n",
    "    for i in (range(0, len(tokens), max_tokens)):\n",
    "        chunk_tokens = tokens[i:i + max_tokens]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "        chunks.append(chunk_text)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Decompose the dataset\n",
    "decomposed_dataset = []\n",
    "for article in tqdm(random_articles, desc=\"Decomposing articles\"):\n",
    "    title = article['title']\n",
    "    content = article['content']\n",
    "    content_chunks = split_content_into_chunks(content, max_tokens=2000)\n",
    "\n",
    "    for idx, chunk in enumerate(content_chunks):\n",
    "        decomposed_dataset.append({\n",
    "            'title': f\"{title} (Part {idx + 1})\",\n",
    "            'content': chunk\n",
    "        })\n",
    "\n",
    "# Save the decomposed dataset\n",
    "with open('summaries/2k_wikipedia_articles.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(decomposed_dataset, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Decomposed dataset saved to 'decomposed_wikipedia_articles.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "my_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
