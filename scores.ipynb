{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import bert_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bertscore_fast(reference_summary, generated_summary, model_type=\"distilbert-base-uncased\"):\n",
    "    P, R, F1 = bert_score.score([generated_summary], [reference_summary], model_type=model_type)\n",
    "    return {\"Precision\": P.item(), \"Recall\": R.item(), \"F1-score\": F1.item()}\n",
    "\n",
    "articles = json.load(open(\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/summaries/2k_finetuned_dpo_summarized_wikipedia_articles.json\"))\n",
    "\n",
    "scores = []\n",
    "\n",
    "\n",
    "for article in tqdm(articles):\n",
    "    reference_summary = article[\"content\"]\n",
    "    good_summary = article[\"summary\"]\n",
    "\n",
    "    score = compute_bertscore_fast(reference_summary, good_summary)\n",
    "    scores.append(score)\n",
    "\n",
    "json.dump(scores, open(\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/bert_scores/dpo_scores_2k\", \"w\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(scores, open(\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/bert_scores/dpo_scores_2k.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores for 8k tokens documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m good_scores \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores/good_scores.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      2\u001b[0m bad_scores \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores/bad_scores.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m dpo_scores \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores/dpo_scores.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "good_scores = json.load(open('bert_scores/good_scores.json', 'r'))\n",
    "bad_scores = json.load(open('bert_scores/bad_scores.json', 'r'))\n",
    "dpo_scores = json.load(open('bert_scores/dpo_scores.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_good_scores = [score['F1-score'] for score in good_scores]\n",
    "F1_bad_scores = [score['F1-score'] for score in bad_scores]\n",
    "F1_dpo_scores = [score['F1-score'] for score in dpo_scores]\n",
    "\n",
    "Precision_good_scores = [score['Precision'] for score in good_scores]\n",
    "Precision_bad_scores = [score['Precision'] for score in bad_scores]\n",
    "Precision_dpo_scores = [score['Precision'] for score in dpo_scores]\n",
    "\n",
    "Recall_good_scores = [score['Recall'] for score in good_scores]\n",
    "Recall_bad_scores = [score['Recall'] for score in bad_scores]\n",
    "Recall_dpo_scores = [score['Recall'] for score in dpo_scores]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-scores:\n",
      "--------------------\n",
      "Good scores mean: 0.6820049513083707\n",
      "\n",
      "Bad scores mean: 0.6423906561939934\n",
      "\n",
      "DPO scores mean: 0.7699803263281942\n",
      "--------------------\n",
      "Precision:\n",
      "--------------------\n",
      "Good scores mean: 0.7453625507224461\n",
      "\n",
      "Bad scores mean: 0.7014457143989976\n",
      "\n",
      "DPO scores mean: 0.8150254339070526\n",
      "--------------------\n",
      "Recall:\n",
      "--------------------\n",
      "Good scores mean: 0.6310297233010365\n",
      "\n",
      "Bad scores mean: 0.5946983401547257\n",
      "\n",
      "DPO scores mean: 0.7303245136971738\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print('F1-scores:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(F1_good_scores) / len(F1_good_scores))\n",
    "print('\\nBad scores mean:', sum(F1_bad_scores) / len(F1_bad_scores))\n",
    "print(\"\\nDPO scores mean:\", sum(F1_dpo_scores) / len(F1_dpo_scores))\n",
    "print(\"--------------------\")\n",
    "print('Precision:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(Precision_good_scores) / len(Precision_good_scores))\n",
    "print('\\nBad scores mean:', sum(Precision_bad_scores) / len(Precision_bad_scores))\n",
    "print(\"\\nDPO scores mean:\", sum(Precision_dpo_scores) / len(Precision_dpo_scores))\n",
    "print(\"--------------------\")\n",
    "print('Recall:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(Recall_good_scores) / len(Recall_good_scores))\n",
    "print('\\nBad scores mean:', sum(Recall_bad_scores) / len(Recall_bad_scores))\n",
    "print(\"\\nDPO scores mean:\", sum(Recall_dpo_scores) / len(Recall_dpo_scores))\n",
    "print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores for 2k tokens documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-scores:\n",
      "--------------------\n",
      "Good scores mean: 0.8144641416221221\n",
      "\n",
      "Bad scores mean: 0.797419783580296\n",
      "\n",
      "SFT scores mean: 0.8301981940897621\n",
      "\n",
      "DPO scores mean: 0.7956697822039074\n",
      "--------------------\n",
      "Precision:\n",
      "--------------------\n",
      "Good scores mean: 0.8533857606665036\n",
      "\n",
      "Bad scores mean: 0.8363323543695795\n",
      "\n",
      "SFT scores mean: 0.8795502921385778\n",
      "\n",
      "DPO scores mean: 0.8332087007136519\n",
      "--------------------\n",
      "Recall:\n",
      "--------------------\n",
      "Good scores mean: 0.7798641234672673\n",
      "\n",
      "Bad scores mean: 0.7628903748005028\n",
      "\n",
      "SFT scores mean: 0.7873001057722269\n",
      "\n",
      "DPO scores mean: 0.7622420048486199\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "good_scores = json.load(open('scores/good_scores_2k.json', 'r'))\n",
    "bad_scores = json.load(open('scores/bad_scores_2k.json', 'r'))\n",
    "sft_scores = json.load(open('scores/SFT_scores.json', 'r'))\n",
    "dpo_scores = json.load(open('scores/dpo_scores_2k.json', 'r'))\n",
    "\n",
    "\n",
    "F1_good_scores = [score['F1-score'] for score in good_scores]\n",
    "F1_bad_scores = [score['F1-score'] for score in bad_scores]\n",
    "F1_sft_scores = [score['F1-score'] for score in sft_scores]\n",
    "F1_dpo_scores = [score['F1-score'] for score in dpo_scores]\n",
    "\n",
    "Precision_good_scores = [score['Precision'] for score in good_scores]\n",
    "Precision_bad_scores = [score['Precision'] for score in bad_scores]\n",
    "Precision_sft_scores = [score['Precision'] for score in sft_scores]\n",
    "Precision_dpo_scores = [score['Precision'] for score in dpo_scores]\n",
    "\n",
    "Recall_good_scores = [score['Recall'] for score in good_scores]\n",
    "Recall_bad_scores = [score['Recall'] for score in bad_scores]\n",
    "Recall_sft_scores = [score['Recall'] for score in sft_scores]\n",
    "Recall_dpo_scores = [score['Recall'] for score in dpo_scores]\n",
    "\n",
    "print('F1-scores:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(F1_good_scores) / len(F1_good_scores))\n",
    "print('\\nBad scores mean:', sum(F1_bad_scores) / len(F1_bad_scores))\n",
    "print(\"\\nSFT scores mean:\", sum(F1_sft_scores) / len(F1_sft_scores))\n",
    "print(\"\\nDPO scores mean:\", sum(F1_dpo_scores) / len(F1_dpo_scores))\n",
    "print(\"--------------------\")\n",
    "print('Precision:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(Precision_good_scores) / len(Precision_good_scores))\n",
    "print('\\nBad scores mean:', sum(Precision_bad_scores) / len(Precision_bad_scores))\n",
    "print(\"\\nSFT scores mean:\", sum(Precision_sft_scores) / len(Precision_sft_scores))\n",
    "print(\"\\nDPO scores mean:\", sum(Precision_dpo_scores) / len(Precision_dpo_scores))\n",
    "print(\"--------------------\")\n",
    "print('Recall:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(Recall_good_scores) / len(Recall_good_scores))\n",
    "print('\\nBad scores mean:', sum(Recall_bad_scores) / len(Recall_bad_scores))\n",
    "print(\"\\nSFT scores mean:\", sum(Recall_sft_scores) / len(Recall_sft_scores))\n",
    "print(\"\\nDPO scores mean:\", sum(Recall_dpo_scores) / len(Recall_dpo_scores))\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposing the scores into training and testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = json.load(open(\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/data/train.json\"))\n",
    "\n",
    "good_scores = json.load(open('scores/good_scores.json', 'r'))\n",
    "bad_scores = json.load(open('scores/bad_scores.json', 'r'))\n",
    "dpo_scores = json.load(open('scores/dpo_scores.json', 'r'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-scores on the training set:\n",
      "--------------------\n",
      "Good scores mean: 0.6822357793154603\n",
      "\n",
      "Bad scores mean: 0.6427095072441382\n",
      "\n",
      "DPO scores mean: 0.7699586713280757\n"
     ]
    }
   ],
   "source": [
    "# Compute the average F1-score for each model on the training set\n",
    "good_scores_train = good_scores[:len(train)]\n",
    "bad_scores_train = bad_scores[:len(train)]\n",
    "dpo_scores_train = dpo_scores[:len(train)]\n",
    "\n",
    "F1_good_scores_train = [score['F1-score'] for score in good_scores_train]\n",
    "F1_bad_scores_train = [score['F1-score'] for score in bad_scores_train]\n",
    "F1_dpo_scores_train = [score['F1-score'] for score in dpo_scores_train]\n",
    "\n",
    "print('F1-scores on the training set:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(F1_good_scores_train) / len(F1_good_scores_train))\n",
    "print('\\nBad scores mean:', sum(F1_bad_scores_train) / len(F1_bad_scores_train))\n",
    "print(\"\\nDPO scores mean:\", sum(F1_dpo_scores_train) / len(F1_dpo_scores_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-scores on the test set:\n",
      "--------------------\n",
      "Good scores mean: 0.6810821079663214\n",
      "\n",
      "Bad scores mean: 0.6411158994067139\n",
      "\n",
      "DPO scores mean: 0.770066902359125\n"
     ]
    }
   ],
   "source": [
    "# Compute the average F1-score for each model on the test set\n",
    "good_scores_test = good_scores[len(train):]\n",
    "bad_scores_test = bad_scores[len(train):]\n",
    "dpo_scores_test = dpo_scores[len(train):]\n",
    "\n",
    "F1_good_scores_test = [score['F1-score'] for score in good_scores_test]\n",
    "F1_bad_scores_test = [score['F1-score'] for score in bad_scores_test]\n",
    "F1_dpo_scores_test = [score['F1-score'] for score in dpo_scores_test]\n",
    "\n",
    "print('F1-scores on the test set:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(F1_good_scores_test) / len(F1_good_scores_test))\n",
    "print('\\nBad scores mean:', sum(F1_bad_scores_test) / len(F1_bad_scores_test))\n",
    "print(\"\\nDPO scores mean:\", sum(F1_dpo_scores_test) / len(F1_dpo_scores_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score for 2k tokens documents Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = json.load(open(\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/data_2k_tokens/train.json\"))\n",
    "\n",
    "good_scores = json.load(open('/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/scores/good_scores_2k.json', 'r'))\n",
    "bad_scores = json.load(open('/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/scores/bad_scores_2k.json', 'r'))\n",
    "dpo_scores = json.load(open('/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/scores/dpo_scores_2k.json', 'r'))\n",
    "sft_scores = json.load(open('/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/scores/SFT_scores.json', 'r'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-scores on the training set:\n",
      "--------------------\n",
      "Good scores mean: 0.8149522696218867\n",
      "\n",
      "Bad scores mean: 0.7978152289165082\n",
      "\n",
      "DPO scores mean: 0.7954185042773022\n",
      "\n",
      "SFT scores mean: 0.8305094244304008\n"
     ]
    }
   ],
   "source": [
    "good_scores_train = good_scores[:len(train)]\n",
    "bad_scores_train = bad_scores[:len(train)]\n",
    "dpo_scores_train = dpo_scores[:len(train)]\n",
    "sft_scores_train = sft_scores[:len(train)]\n",
    "\n",
    "F1_good_scores_train = [score['F1-score'] for score in good_scores_train]\n",
    "F1_bad_scores_train = [score['F1-score'] for score in bad_scores_train]\n",
    "F1_dpo_scores_train = [score['F1-score'] for score in dpo_scores_train]\n",
    "F1_sft_scores_train = [score['F1-score'] for score in sft_scores_train]\n",
    "\n",
    "print('F1-scores on the training set:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(F1_good_scores_train) / len(F1_good_scores_train))\n",
    "print('\\nBad scores mean:', sum(F1_bad_scores_train) / len(F1_bad_scores_train))\n",
    "print(\"\\nDPO scores mean:\", sum(F1_dpo_scores_train) / len(F1_dpo_scores_train))\n",
    "print(\"\\nSFT scores mean:\", sum(F1_sft_scores_train) / len(F1_sft_scores_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-scores on the test set:\n",
      "--------------------\n",
      "Good scores mean: 0.813325341728416\n",
      "\n",
      "Bad scores mean: 0.7964972118223508\n",
      "\n",
      "DPO scores mean: 0.7962738232752489\n",
      "\n",
      "SFT scores mean: 0.8294500348266973\n"
     ]
    }
   ],
   "source": [
    "good_scores_test = good_scores[len(train):]\n",
    "bad_scores_test = bad_scores[len(train):]\n",
    "dpo_scores_test = dpo_scores[len(train):]\n",
    "sft_scores_test = sft_scores[len(train):]\n",
    "\n",
    "F1_good_scores_test = [score['F1-score'] for score in good_scores_test]\n",
    "F1_bad_scores_test = [score['F1-score'] for score in bad_scores_test]\n",
    "F1_dpo_scores_test = [score['F1-score'] for score in dpo_scores_test]\n",
    "F1_sft_scores_test = [score['F1-score'] for score in sft_scores_test]\n",
    "\n",
    "print('F1-scores on the test set:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(F1_good_scores_test) / len(F1_good_scores_test))\n",
    "print('\\nBad scores mean:', sum(F1_bad_scores_test) / len(F1_bad_scores_test))\n",
    "print(\"\\nDPO scores mean:\", sum(F1_dpo_scores_test) / len(F1_dpo_scores_test))\n",
    "print(\"\\nSFT scores mean:\", sum(F1_sft_scores_test) / len(F1_sft_scores_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rouge Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6498/6498 [00:52<00:00, 124.40it/s]\n"
     ]
    }
   ],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "articles = json.load(open(\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/summaries/2k_finetuned_summarized_wikipedia_articles.json\"))\n",
    "\n",
    "# Compute the ROUGE scores for each article\n",
    "\n",
    "scores = []\n",
    "\n",
    "for article in tqdm(articles):\n",
    "    reference_summary = article[\"content\"]\n",
    "    summary = article[\"summary\"]\n",
    "\n",
    "    scores.append(scorer.score(reference_summary, summary))\n",
    "\n",
    "json.dump(scores, open(\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/rouge_scores/sft_scores_2k.json\", \"w\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores for 8k tokens documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_scores = json.load(open('/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/rouge_scores/good_scores_8k.json', 'r'))\n",
    "bad_scores = json.load(open('/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/rouge_scores/bad_scores_8k.json', 'r'))\n",
    "dpo_scores = json.load(open('/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/rouge_scores/dpo_scores_8k.json', 'r'))\n",
    "\n",
    "train_size = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rouge-1 F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 scores:\n",
      "Training scores:\n",
      "--------------------\n",
      "Good scores mean: 0.7664347888103062\n",
      "\n",
      "Bad scores mean: 0.6004878160512062\n",
      "\n",
      "DPO scores mean: 0.5845655220444853\n",
      "--------------------\n",
      "Test scores:\n",
      "--------------------\n",
      "Good scores mean: 0.7686514268790634\n",
      "\n",
      "Bad scores mean: 0.5966565224470679\n",
      "\n",
      "DPO scores mean: 0.5940162064000093\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "rouge1_good_scores = [score['rouge1'][0] for score in good_scores]\n",
    "rouge1_bad_scores = [score['rouge1'][0] for score in bad_scores]\n",
    "rouge1_dpo_scores = [score['rouge1'][0] for score in dpo_scores]\n",
    "print('ROUGE-1 scores:')\n",
    "print(\"Training scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rouge1_good_scores[:int(train_size * len(rouge1_good_scores))]) / len(rouge1_good_scores[:int(train_size * len(rouge1_good_scores))]))\n",
    "print('\\nBad scores mean:', sum(rouge1_bad_scores[:int(train_size * len(rouge1_bad_scores))]) / len(rouge1_bad_scores[:int(train_size * len(rouge1_bad_scores))]))\n",
    "print(\"\\nDPO scores mean:\", sum(rouge1_dpo_scores[:int(train_size * len(rouge1_dpo_scores))]) / len(rouge1_dpo_scores[:int(train_size * len(rouge1_dpo_scores))]))\n",
    "print(\"--------------------\")\n",
    "print(\"Test scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rouge1_good_scores[int(train_size * len(rouge1_good_scores)):]) / len(rouge1_good_scores[int(train_size * len(rouge1_good_scores)):]))\n",
    "print('\\nBad scores mean:', sum(rouge1_bad_scores[int(train_size * len(rouge1_bad_scores)):]) / len(rouge1_bad_scores[int(train_size * len(rouge1_bad_scores)):]))\n",
    "print(\"\\nDPO scores mean:\", sum(rouge1_dpo_scores[int(train_size * len(rouge1_dpo_scores)):]) / len(rouge1_dpo_scores[int(train_size * len(rouge1_dpo_scores)):]))\n",
    "print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rouge-2 F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-2 scores:\n",
      "Training scores:\n",
      "--------------------\n",
      "Good scores mean: 0.4314135826130457\n",
      "\n",
      "Bad scores mean: 0.3201015425891059\n",
      "\n",
      "DPO scores mean: 0.21517255965759072\n",
      "--------------------\n",
      "Test scores:\n",
      "--------------------\n",
      "Good scores mean: 0.4281350415985045\n",
      "\n",
      "Bad scores mean: 0.3121613461177329\n",
      "\n",
      "DPO scores mean: 0.21478939464611416\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "rouge2_good_scores = [score['rouge2'][0] for score in good_scores]\n",
    "rouge2_bad_scores = [score['rouge2'][0] for score in bad_scores]\n",
    "rouge2_dpo_scores = [score['rouge2'][0] for score in dpo_scores]\n",
    "print('ROUGE-2 scores:')\n",
    "print(\"Training scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rouge2_good_scores[:int(train_size * len(rouge2_good_scores))]) / len(rouge2_good_scores[:int(train_size * len(rouge2_good_scores))]))\n",
    "print('\\nBad scores mean:', sum(rouge2_bad_scores[:int(train_size * len(rouge2_bad_scores))]) / len(rouge2_bad_scores[:int(train_size * len(rouge2_bad_scores))]))\n",
    "print(\"\\nDPO scores mean:\", sum(rouge2_dpo_scores[:int(train_size * len(rouge2_dpo_scores))]) / len(rouge2_dpo_scores[:int(train_size * len(rouge2_dpo_scores))]))\n",
    "print(\"--------------------\")\n",
    "print(\"Test scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rouge2_good_scores[int(train_size * len(rouge2_good_scores)):]) / len(rouge2_good_scores[int(train_size * len(rouge2_good_scores)):]))\n",
    "print('\\nBad scores mean:', sum(rouge2_bad_scores[int(train_size * len(rouge2_bad_scores)):]) / len(rouge2_bad_scores[int(train_size * len(rouge2_bad_scores)):]))\n",
    "print(\"\\nDPO scores mean:\", sum(rouge2_dpo_scores[int(train_size * len(rouge2_dpo_scores)):]) / len(rouge2_dpo_scores[int(train_size * len(rouge2_dpo_scores)):]))\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rouge-L F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-L scores:\n",
      "Training scores:\n",
      "--------------------\n",
      "Good scores mean: 0.5862384824429212\n",
      "\n",
      "Bad scores mean: 0.43570175833313923\n",
      "\n",
      "DPO scores mean: 0.4197006638988147\n",
      "--------------------\n",
      "Test scores:\n",
      "--------------------\n",
      "Good scores mean: 0.5829627142829823\n",
      "\n",
      "Bad scores mean: 0.4297396676295664\n",
      "\n",
      "DPO scores mean: 0.425424527155304\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "rougeL_good_scores = [score['rougeL'][0] for score in good_scores]\n",
    "rougeL_bad_scores = [score['rougeL'][0] for score in bad_scores]\n",
    "rougeL_dpo_scores = [score['rougeL'][0] for score in dpo_scores]\n",
    "print('ROUGE-L scores:')\n",
    "print(\"Training scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rougeL_good_scores[:int(train_size * len(rougeL_good_scores))]) / len(rougeL_good_scores[:int(train_size * len(rougeL_good_scores))]))\n",
    "print('\\nBad scores mean:', sum(rougeL_bad_scores[:int(train_size * len(rougeL_bad_scores))]) / len(rougeL_bad_scores[:int(train_size * len(rougeL_bad_scores))]))\n",
    "print(\"\\nDPO scores mean:\", sum(rougeL_dpo_scores[:int(train_size * len(rougeL_dpo_scores))]) / len(rougeL_dpo_scores[:int(train_size * len(rougeL_dpo_scores))]))\n",
    "print(\"--------------------\")\n",
    "print(\"Test scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rougeL_good_scores[int(train_size * len(rougeL_good_scores)):]) / len(rougeL_good_scores[int(train_size * len(rougeL_good_scores)):]))\n",
    "print('\\nBad scores mean:', sum(rougeL_bad_scores[int(train_size * len(rougeL_bad_scores)):]) / len(rougeL_bad_scores[int(train_size * len(rougeL_bad_scores)):]))\n",
    "print(\"\\nDPO scores mean:\", sum(rougeL_dpo_scores[int(train_size * len(rougeL_dpo_scores)):]) / len(rougeL_dpo_scores[int(train_size * len(rougeL_dpo_scores)):]))\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores for 2k tokens documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_scores = json.load(open('/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/rouge_scores/good_scores_2k.json', 'r'))\n",
    "bad_scores = json.load(open('/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/rouge_scores/bad_scores_2k.json', 'r'))\n",
    "dpo_scores = json.load(open('/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/rouge_scores/dpo_scores_2k.json', 'r'))\n",
    "sft_scores = json.load(open('/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/rouge_scores/sft_scores_2k.json', 'r'))\n",
    "\n",
    "train_size = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rouge-1 F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 scores:\n",
      "Training scores:\n",
      "--------------------\n",
      "Good scores mean: 0.751753031874592\n",
      "\n",
      "Bad scores mean: 0.6080961269545818\n",
      "\n",
      "DPO scores mean: 0.5947431005397751\n",
      "\n",
      "SFT scores mean: 0.7892345105566898\n",
      "--------------------\n",
      "Test scores:\n",
      "--------------------\n",
      "Good scores mean: 0.751103870514794\n",
      "\n",
      "Bad scores mean: 0.6147772922826621\n",
      "\n",
      "DPO scores mean: 0.5875321861362157\n",
      "\n",
      "SFT scores mean: 0.7892884830368279\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "rouge1_good_scores = [score['rouge1'][0] for score in good_scores]\n",
    "rouge1_bad_scores = [score['rouge1'][0] for score in bad_scores]\n",
    "rouge1_dpo_scores = [score['rouge1'][0] for score in dpo_scores]\n",
    "rouge1_sft_scores = [score['rouge1'][0] for score in sft_scores]\n",
    "print('ROUGE-1 scores:')\n",
    "print(\"Training scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rouge1_good_scores[:int(train_size * len(rouge1_good_scores))]) / len(rouge1_good_scores[:int(train_size * len(rouge1_good_scores))]))\n",
    "print('\\nBad scores mean:', sum(rouge1_bad_scores[:int(train_size * len(rouge1_bad_scores))]) / len(rouge1_bad_scores[:int(train_size * len(rouge1_bad_scores))]))\n",
    "print(\"\\nDPO scores mean:\", sum(rouge1_dpo_scores[:int(train_size * len(rouge1_dpo_scores))]) / len(rouge1_dpo_scores[:int(train_size * len(rouge1_dpo_scores))]))\n",
    "print(\"\\nSFT scores mean:\", sum(rouge1_sft_scores[:int(train_size * len(rouge1_sft_scores))]) / len(rouge1_sft_scores[:int(train_size * len(rouge1_sft_scores))]))\n",
    "print(\"--------------------\")\n",
    "print(\"Test scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rouge1_good_scores[int(train_size * len(rouge1_good_scores)):]) / len(rouge1_good_scores[int(train_size * len(rouge1_good_scores)):]))\n",
    "print('\\nBad scores mean:', sum(rouge1_bad_scores[int(train_size * len(rouge1_bad_scores)):]) / len(rouge1_bad_scores[int(train_size * len(rouge1_bad_scores)):]))\n",
    "print(\"\\nDPO scores mean:\", sum(rouge1_dpo_scores[int(train_size * len(rouge1_dpo_scores)):]) / len(rouge1_dpo_scores[int(train_size * len(rouge1_dpo_scores)):]))\n",
    "print(\"\\nSFT scores mean:\", sum(rouge1_sft_scores[int(train_size * len(rouge1_sft_scores)):]) / len(rouge1_sft_scores[int(train_size * len(rouge1_sft_scores)):]))\n",
    "print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rouge-2 F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-2 scores:\n",
      "Training scores:\n",
      "--------------------\n",
      "Good scores mean: 0.39880191527739184\n",
      "\n",
      "Bad scores mean: 0.3396875945166942\n",
      "\n",
      "DPO scores mean: 0.3011038206105337\n",
      "\n",
      "SFT scores mean: 0.5417822128371445\n",
      "--------------------\n",
      "Test scores:\n",
      "--------------------\n",
      "Good scores mean: 0.3956736961852618\n",
      "\n",
      "Bad scores mean: 0.3438119033575253\n",
      "\n",
      "DPO scores mean: 0.2950888306599134\n",
      "\n",
      "SFT scores mean: 0.5359303784791478\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "rouge2_good_scores = [score['rouge2'][0] for score in good_scores]\n",
    "rouge2_bad_scores = [score['rouge2'][0] for score in bad_scores]\n",
    "rouge2_dpo_scores = [score['rouge2'][0] for score in dpo_scores]\n",
    "rouge2_sft_scores = [score['rouge2'][0] for score in sft_scores]\n",
    "print('ROUGE-2 scores:')\n",
    "print(\"Training scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rouge2_good_scores[:int(train_size * len(rouge2_good_scores))]) / len(rouge2_good_scores[:int(train_size * len(rouge2_good_scores))]))\n",
    "print('\\nBad scores mean:', sum(rouge2_bad_scores[:int(train_size * len(rouge2_bad_scores))]) / len(rouge2_bad_scores[:int(train_size * len(rouge2_bad_scores))]))\n",
    "print(\"\\nDPO scores mean:\", sum(rouge2_dpo_scores[:int(train_size * len(rouge2_dpo_scores))]) / len(rouge2_dpo_scores[:int(train_size * len(rouge2_dpo_scores))]))\n",
    "print(\"\\nSFT scores mean:\", sum(rouge2_sft_scores[:int(train_size * len(rouge2_sft_scores))]) / len(rouge2_sft_scores[:int(train_size * len(rouge2_sft_scores))]))\n",
    "print(\"--------------------\")\n",
    "print(\"Test scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rouge2_good_scores[int(train_size * len(rouge2_good_scores)):]) / len(rouge2_good_scores[int(train_size * len(rouge2_good_scores)):]))\n",
    "print('\\nBad scores mean:', sum(rouge2_bad_scores[int(train_size * len(rouge2_bad_scores)):]) / len(rouge2_bad_scores[int(train_size * len(rouge2_bad_scores)):]))\n",
    "print(\"\\nDPO scores mean:\", sum(rouge2_dpo_scores[int(train_size * len(rouge2_dpo_scores)):]) / len(rouge2_dpo_scores[int(train_size * len(rouge2_dpo_scores)):]))\n",
    "print(\"\\nSFT scores mean:\", sum(rouge2_sft_scores[int(train_size * len(rouge2_sft_scores)):]) / len(rouge2_sft_scores[int(train_size * len(rouge2_sft_scores)):]))\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rouge-L F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-L scores:\n",
      "Training scores:\n",
      "--------------------\n",
      "Good scores mean: 0.5562012254199095\n",
      "\n",
      "Bad scores mean: 0.44100793324710685\n",
      "\n",
      "DPO scores mean: 0.4204164376359979\n",
      "\n",
      "SFT scores mean: 0.6245999721312844\n",
      "--------------------\n",
      "Test scores:\n",
      "--------------------\n",
      "Good scores mean: 0.5546049449790932\n",
      "\n",
      "Bad scores mean: 0.4424617051811035\n",
      "\n",
      "DPO scores mean: 0.41436384195624537\n",
      "\n",
      "SFT scores mean: 0.6191841510223639\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "rougeL_good_scores = [score['rougeL'][0] for score in good_scores]\n",
    "rougeL_bad_scores = [score['rougeL'][0] for score in bad_scores]\n",
    "rougeL_dpo_scores = [score['rougeL'][0] for score in dpo_scores]\n",
    "rougeL_sft_scores = [score['rougeL'][0] for score in sft_scores]\n",
    "print('ROUGE-L scores:')\n",
    "print(\"Training scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rougeL_good_scores[:int(train_size * len(rougeL_good_scores))]) / len(rougeL_good_scores[:int(train_size * len(rougeL_good_scores))]))\n",
    "print('\\nBad scores mean:', sum(rougeL_bad_scores[:int(train_size * len(rougeL_bad_scores))]) / len(rougeL_bad_scores[:int(train_size * len(rougeL_bad_scores))]))\n",
    "print(\"\\nDPO scores mean:\", sum(rougeL_dpo_scores[:int(train_size * len(rougeL_dpo_scores))]) / len(rougeL_dpo_scores[:int(train_size * len(rougeL_dpo_scores))]))\n",
    "print(\"\\nSFT scores mean:\", sum(rougeL_sft_scores[:int(train_size * len(rougeL_sft_scores))]) / len(rougeL_sft_scores[:int(train_size * len(rougeL_sft_scores))]))\n",
    "print(\"--------------------\")\n",
    "print(\"Test scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rougeL_good_scores[int(train_size * len(rougeL_good_scores)):]) / len(rougeL_good_scores[int(train_size * len(rougeL_good_scores)):]))\n",
    "print('\\nBad scores mean:', sum(rougeL_bad_scores[int(train_size * len(rougeL_bad_scores)):]) / len(rougeL_bad_scores[int(train_size * len(rougeL_bad_scores)):]))\n",
    "print(\"\\nDPO scores mean:\", sum(rougeL_dpo_scores[int(train_size * len(rougeL_dpo_scores)):]) / len(rougeL_dpo_scores[int(train_size * len(rougeL_dpo_scores)):]))\n",
    "print(\"\\nSFT scores mean:\", sum(rougeL_sft_scores[int(train_size * len(rougeL_sft_scores)):]) / len(rougeL_sft_scores[int(train_size * len(rougeL_sft_scores)):]))\n",
    "print(\"--------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "my_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
