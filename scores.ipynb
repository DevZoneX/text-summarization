{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the summaries\n",
    "token_size = \"2k\" # or 8k\n",
    "\n",
    "good_summaries = json.load(open(f\"summaries/{token_size}_good_summarized_wikipedia_articles.json\"))\n",
    "bad_summaries = json.load(open(f\"summaries/{token_size}_bad_summarized_wikipedia_articles.json\"))\n",
    "dpo_summaries = json.load(open(f\"summaries/{token_size}_finetuned_dpo_summarized_wikipedia_articles.json\"))\n",
    "\n",
    "if token_size == \"2k\":\n",
    "    sft_summaries = json.load(open(f\"summaries/{token_size}_finetuned_sft_summarized_wikipedia_articles.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the scores and saving them in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bertscore_fast(reference_summary, generated_summary, model_type=\"distilbert-base-uncased\"):\n",
    "    P, R, F1 = bert_score.score([generated_summary], [reference_summary], model_type=model_type)\n",
    "    return {\"Precision\": P.item(), \"Recall\": R.item(), \"F1-score\": F1.item()}\n",
    "\n",
    "good_scores = []\n",
    "bad_scores = []\n",
    "dpo_scores = []\n",
    "sft_scores = []\n",
    "\n",
    "for i in tqdm(range(len(good_summaries))):\n",
    "    good_scores.append(compute_bertscore_fast(good_summaries[i], good_summaries[i]))\n",
    "    bad_scores.append(compute_bertscore_fast(good_summaries[i], bad_summaries[i]))\n",
    "    dpo_scores.append(compute_bertscore_fast(good_summaries[i], dpo_summaries[i]))\n",
    "    if token_size == \"2k\":\n",
    "        sft_scores.append(compute_bertscore_fast(good_summaries[i], sft_summaries[i]))\n",
    "\n",
    "# Saving the scores\n",
    "json.dump(good_scores, open(f\"bert_scores/{token_size}_good_scores.json\", \"w\"))\n",
    "json.dump(bad_scores, open(f\"bert_scores/{token_size}_bad_scores.json\", \"w\"))\n",
    "json.dump(dpo_scores, open(f\"bert_scores/{token_size}_dpo_scores.json\", \"w\"))\n",
    "if token_size == \"2k\":\n",
    "    json.dump(sft_scores, open(f\"bert_scores/{token_size}_sft_scores.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores for 8k tokens documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_scores = json.load(open('bert_scores/good_scores_8k.json', 'r'))\n",
    "bad_scores = json.load(open('bert_scores/bad_scores_8k.json', 'r'))\n",
    "dpo_scores = json.load(open('bert_scores/dpo_scores_8k.json', 'r'))\n",
    "\n",
    "train_size = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-scores on the training set:\n",
      "--------------------\n",
      "Good scores mean: 0.6822357793154603\n",
      "\n",
      "Bad scores mean: 0.6427095072441382\n",
      "\n",
      "DPO scores mean: 0.7699586713280757\n",
      "\n",
      "\n",
      "F1-scores on the test set:\n",
      "--------------------\n",
      "Good scores mean: 0.6810821079663214\n",
      "\n",
      "Bad scores mean: 0.6411158994067139\n",
      "\n",
      "DPO scores mean: 0.770066902359125\n"
     ]
    }
   ],
   "source": [
    "# Compute the average F1-score for each model on the training set\n",
    "good_scores_train = good_scores[:int(train_size * len(good_scores))]\n",
    "bad_scores_train = bad_scores[:int(train_size * len(good_scores))]\n",
    "dpo_scores_train = dpo_scores[:int(train_size * len(good_scores))]\n",
    "\n",
    "F1_good_scores_train = [score['F1-score'] for score in good_scores_train]\n",
    "F1_bad_scores_train = [score['F1-score'] for score in bad_scores_train]\n",
    "F1_dpo_scores_train = [score['F1-score'] for score in dpo_scores_train]\n",
    "\n",
    "print('F1-scores on the training set:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(F1_good_scores_train) / len(F1_good_scores_train))\n",
    "print('\\nBad scores mean:', sum(F1_bad_scores_train) / len(F1_bad_scores_train))\n",
    "print(\"\\nDPO scores mean:\", sum(F1_dpo_scores_train) / len(F1_dpo_scores_train))\n",
    "print(\"\\n\")\n",
    "# Compute the average F1-score for each model on the test set\n",
    "good_scores_test = good_scores[int(train_size * len(good_scores)):]\n",
    "bad_scores_test = bad_scores[int(train_size * len(good_scores)):]\n",
    "dpo_scores_test = dpo_scores[int(train_size * len(good_scores)):]\n",
    "\n",
    "F1_good_scores_test = [score['F1-score'] for score in good_scores_test]\n",
    "F1_bad_scores_test = [score['F1-score'] for score in bad_scores_test]\n",
    "F1_dpo_scores_test = [score['F1-score'] for score in dpo_scores_test]\n",
    "\n",
    "print('F1-scores on the test set:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(F1_good_scores_test) / len(F1_good_scores_test))\n",
    "print('\\nBad scores mean:', sum(F1_bad_scores_test) / len(F1_bad_scores_test))\n",
    "print(\"\\nDPO scores mean:\", sum(F1_dpo_scores_test) / len(F1_dpo_scores_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores for 2k tokens documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "good_scores = json.load(open('bert_scores/good_scores_2k.json', 'r'))\n",
    "bad_scores = json.load(open('bert_scores/bad_scores_2k.json', 'r'))\n",
    "sft_scores = json.load(open('bert_scores/sft_scores_2k.json', 'r'))\n",
    "dpo_scores = json.load(open('bert_scores/dpo_scores_2k.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-scores on the training set:\n",
      "--------------------\n",
      "Good scores mean: 0.8148783289185924\n",
      "\n",
      "Bad scores mean: 0.7971492961424166\n",
      "\n",
      "DPO scores mean: 0.7965072462694687\n",
      "\n",
      "SFT scores mean: 0.8304813184599945\n",
      "\n",
      "\n",
      "F1-scores on the test set:\n",
      "--------------------\n",
      "Good scores mean: 0.8128086552023888\n",
      "\n",
      "Bad scores mean: 0.7985009086749902\n",
      "\n",
      "DPO scores mean: 0.7946119556909564\n",
      "\n",
      "SFT scores mean: 0.8290142194506084\n"
     ]
    }
   ],
   "source": [
    "# Compute the average F1-score for each model on the training set\n",
    "good_scores_train = good_scores[:int(train_size * len(good_scores))]\n",
    "bad_scores_train = bad_scores[:int(train_size * len(good_scores))]\n",
    "dpo_scores_train = dpo_scores[:int(train_size * len(good_scores))]\n",
    "sft_scores_train = sft_scores[:int(train_size * len(good_scores))]\n",
    "\n",
    "F1_good_scores_train = [score['F1-score'] for score in good_scores_train]\n",
    "F1_bad_scores_train = [score['F1-score'] for score in bad_scores_train]\n",
    "F1_dpo_scores_train = [score['F1-score'] for score in dpo_scores_train]\n",
    "F1_sft_scores_train = [score['F1-score'] for score in sft_scores_train]\n",
    "\n",
    "print('F1-scores on the training set:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(F1_good_scores_train) / len(F1_good_scores_train))\n",
    "print('\\nBad scores mean:', sum(F1_bad_scores_train) / len(F1_bad_scores_train))\n",
    "print(\"\\nDPO scores mean:\", sum(F1_dpo_scores_train) / len(F1_dpo_scores_train))\n",
    "print(\"\\nSFT scores mean:\", sum(F1_sft_scores_train) / len(F1_sft_scores_train))\n",
    "print(\"\\n\")\n",
    "# Compute the average F1-score for each model on the test set\n",
    "good_scores_test = good_scores[int(train_size * len(good_scores)):]\n",
    "bad_scores_test = bad_scores[int(train_size * len(good_scores)):]\n",
    "dpo_scores_test = dpo_scores[int(train_size * len(good_scores)):]\n",
    "sft_scores_test = sft_scores[int(train_size * len(good_scores)):]\n",
    "\n",
    "F1_good_scores_test = [score['F1-score'] for score in good_scores_test]\n",
    "F1_bad_scores_test = [score['F1-score'] for score in bad_scores_test]\n",
    "F1_dpo_scores_test = [score['F1-score'] for score in dpo_scores_test]\n",
    "F1_sft_scores_test = [score['F1-score'] for score in sft_scores_test]\n",
    "\n",
    "print('F1-scores on the test set:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(F1_good_scores_test) / len(F1_good_scores_test))\n",
    "print('\\nBad scores mean:', sum(F1_bad_scores_test) / len(F1_bad_scores_test))\n",
    "print(\"\\nDPO scores mean:\", sum(F1_dpo_scores_test) / len(F1_dpo_scores_test))\n",
    "print(\"\\nSFT scores mean:\", sum(F1_sft_scores_test) / len(F1_sft_scores_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rouge Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6498/6498 [00:52<00:00, 124.40it/s]\n"
     ]
    }
   ],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "good_scores = []\n",
    "bad_scores = []\n",
    "dpo_scores = []\n",
    "sft_scores = []\n",
    "\n",
    "for i in tqdm(range(len(good_summaries))):\n",
    "    good_scores.append(scorer.score(good_summaries[i], good_summaries[i]))\n",
    "    bad_scores.append(scorer.score(good_summaries[i], bad_summaries[i]))\n",
    "    dpo_scores.append(scorer.score(good_summaries[i], dpo_summaries[i]))\n",
    "    if token_size == \"2k\":\n",
    "        sft_scores.append(scorer.score(good_summaries[i], sft_summaries[i]))\n",
    "\n",
    "# Saving the scores\n",
    "json.dump(good_scores, open(f\"rouge_scores/{token_size}_good_scores.json\", \"w\"))\n",
    "json.dump(bad_scores, open(f\"rouge_scores/{token_size}_bad_scores.json\", \"w\"))\n",
    "json.dump(dpo_scores, open(f\"rouge_scores/{token_size}_dpo_scores.json\", \"w\"))\n",
    "if token_size == \"2k\":\n",
    "    json.dump(sft_scores, open(f\"rouge_scores/{token_size}_sft_scores.json\", \"w\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores for 8k tokens documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_scores = json.load(open('rouge_scores/good_scores_8k.json', 'r'))\n",
    "bad_scores = json.load(open('rouge_scores/bad_scores_8k.json', 'r'))\n",
    "dpo_scores = json.load(open('rouge_scores/dpo_scores_8k.json', 'r'))\n",
    "\n",
    "train_size = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rouge-1 F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 scores:\n",
      "Training scores:\n",
      "--------------------\n",
      "Good scores mean: 0.7664347888103062\n",
      "\n",
      "Bad scores mean: 0.6004878160512062\n",
      "\n",
      "DPO scores mean: 0.5845655220444853\n",
      "--------------------\n",
      "Test scores:\n",
      "--------------------\n",
      "Good scores mean: 0.7686514268790634\n",
      "\n",
      "Bad scores mean: 0.5966565224470679\n",
      "\n",
      "DPO scores mean: 0.5940162064000093\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "rouge1_good_scores = [score['rouge1'][0] for score in good_scores]\n",
    "rouge1_bad_scores = [score['rouge1'][0] for score in bad_scores]\n",
    "rouge1_dpo_scores = [score['rouge1'][0] for score in dpo_scores]\n",
    "print('ROUGE-1 scores:')\n",
    "print(\"Training scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rouge1_good_scores[:int(train_size * len(rouge1_good_scores))]) / len(rouge1_good_scores[:int(train_size * len(rouge1_good_scores))]))\n",
    "print('\\nBad scores mean:', sum(rouge1_bad_scores[:int(train_size * len(rouge1_bad_scores))]) / len(rouge1_bad_scores[:int(train_size * len(rouge1_bad_scores))]))\n",
    "print(\"\\nDPO scores mean:\", sum(rouge1_dpo_scores[:int(train_size * len(rouge1_dpo_scores))]) / len(rouge1_dpo_scores[:int(train_size * len(rouge1_dpo_scores))]))\n",
    "print(\"--------------------\")\n",
    "print(\"Test scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rouge1_good_scores[int(train_size * len(rouge1_good_scores)):]) / len(rouge1_good_scores[int(train_size * len(rouge1_good_scores)):]))\n",
    "print('\\nBad scores mean:', sum(rouge1_bad_scores[int(train_size * len(rouge1_bad_scores)):]) / len(rouge1_bad_scores[int(train_size * len(rouge1_bad_scores)):]))\n",
    "print(\"\\nDPO scores mean:\", sum(rouge1_dpo_scores[int(train_size * len(rouge1_dpo_scores)):]) / len(rouge1_dpo_scores[int(train_size * len(rouge1_dpo_scores)):]))\n",
    "print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rouge-2 F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-2 scores:\n",
      "Training scores:\n",
      "--------------------\n",
      "Good scores mean: 0.4314135826130457\n",
      "\n",
      "Bad scores mean: 0.3201015425891059\n",
      "\n",
      "DPO scores mean: 0.21517255965759072\n",
      "--------------------\n",
      "Test scores:\n",
      "--------------------\n",
      "Good scores mean: 0.4281350415985045\n",
      "\n",
      "Bad scores mean: 0.3121613461177329\n",
      "\n",
      "DPO scores mean: 0.21478939464611416\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "rouge2_good_scores = [score['rouge2'][0] for score in good_scores]\n",
    "rouge2_bad_scores = [score['rouge2'][0] for score in bad_scores]\n",
    "rouge2_dpo_scores = [score['rouge2'][0] for score in dpo_scores]\n",
    "print('ROUGE-2 scores:')\n",
    "print(\"Training scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rouge2_good_scores[:int(train_size * len(rouge2_good_scores))]) / len(rouge2_good_scores[:int(train_size * len(rouge2_good_scores))]))\n",
    "print('\\nBad scores mean:', sum(rouge2_bad_scores[:int(train_size * len(rouge2_bad_scores))]) / len(rouge2_bad_scores[:int(train_size * len(rouge2_bad_scores))]))\n",
    "print(\"\\nDPO scores mean:\", sum(rouge2_dpo_scores[:int(train_size * len(rouge2_dpo_scores))]) / len(rouge2_dpo_scores[:int(train_size * len(rouge2_dpo_scores))]))\n",
    "print(\"--------------------\")\n",
    "print(\"Test scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rouge2_good_scores[int(train_size * len(rouge2_good_scores)):]) / len(rouge2_good_scores[int(train_size * len(rouge2_good_scores)):]))\n",
    "print('\\nBad scores mean:', sum(rouge2_bad_scores[int(train_size * len(rouge2_bad_scores)):]) / len(rouge2_bad_scores[int(train_size * len(rouge2_bad_scores)):]))\n",
    "print(\"\\nDPO scores mean:\", sum(rouge2_dpo_scores[int(train_size * len(rouge2_dpo_scores)):]) / len(rouge2_dpo_scores[int(train_size * len(rouge2_dpo_scores)):]))\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rouge-L F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-L scores:\n",
      "Training scores:\n",
      "--------------------\n",
      "Good scores mean: 0.5862384824429212\n",
      "\n",
      "Bad scores mean: 0.43570175833313923\n",
      "\n",
      "DPO scores mean: 0.4197006638988147\n",
      "--------------------\n",
      "Test scores:\n",
      "--------------------\n",
      "Good scores mean: 0.5829627142829823\n",
      "\n",
      "Bad scores mean: 0.4297396676295664\n",
      "\n",
      "DPO scores mean: 0.425424527155304\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "rougeL_good_scores = [score['rougeL'][0] for score in good_scores]\n",
    "rougeL_bad_scores = [score['rougeL'][0] for score in bad_scores]\n",
    "rougeL_dpo_scores = [score['rougeL'][0] for score in dpo_scores]\n",
    "print('ROUGE-L scores:')\n",
    "print(\"Training scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rougeL_good_scores[:int(train_size * len(rougeL_good_scores))]) / len(rougeL_good_scores[:int(train_size * len(rougeL_good_scores))]))\n",
    "print('\\nBad scores mean:', sum(rougeL_bad_scores[:int(train_size * len(rougeL_bad_scores))]) / len(rougeL_bad_scores[:int(train_size * len(rougeL_bad_scores))]))\n",
    "print(\"\\nDPO scores mean:\", sum(rougeL_dpo_scores[:int(train_size * len(rougeL_dpo_scores))]) / len(rougeL_dpo_scores[:int(train_size * len(rougeL_dpo_scores))]))\n",
    "print(\"--------------------\")\n",
    "print(\"Test scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rougeL_good_scores[int(train_size * len(rougeL_good_scores)):]) / len(rougeL_good_scores[int(train_size * len(rougeL_good_scores)):]))\n",
    "print('\\nBad scores mean:', sum(rougeL_bad_scores[int(train_size * len(rougeL_bad_scores)):]) / len(rougeL_bad_scores[int(train_size * len(rougeL_bad_scores)):]))\n",
    "print(\"\\nDPO scores mean:\", sum(rougeL_dpo_scores[int(train_size * len(rougeL_dpo_scores)):]) / len(rougeL_dpo_scores[int(train_size * len(rougeL_dpo_scores)):]))\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores for 2k tokens documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_scores = json.load(open('rouge_scores/good_scores_2k.json', 'r'))\n",
    "bad_scores = json.load(open('rouge_scores/bad_scores_2k.json', 'r'))\n",
    "dpo_scores = json.load(open('rouge_scores/dpo_scores_2k.json', 'r'))\n",
    "sft_scores = json.load(open('rouge_scores/sft_scores_2k.json', 'r'))\n",
    "\n",
    "train_size = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rouge-1 F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 scores:\n",
      "Training scores:\n",
      "--------------------\n",
      "Good scores mean: 0.751753031874592\n",
      "\n",
      "Bad scores mean: 0.6080961269545818\n",
      "\n",
      "DPO scores mean: 0.5947431005397751\n",
      "\n",
      "SFT scores mean: 0.7892345105566898\n",
      "--------------------\n",
      "Test scores:\n",
      "--------------------\n",
      "Good scores mean: 0.751103870514794\n",
      "\n",
      "Bad scores mean: 0.6147772922826621\n",
      "\n",
      "DPO scores mean: 0.5875321861362157\n",
      "\n",
      "SFT scores mean: 0.7892884830368279\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "rouge1_good_scores = [score['rouge1'][0] for score in good_scores]\n",
    "rouge1_bad_scores = [score['rouge1'][0] for score in bad_scores]\n",
    "rouge1_dpo_scores = [score['rouge1'][0] for score in dpo_scores]\n",
    "rouge1_sft_scores = [score['rouge1'][0] for score in sft_scores]\n",
    "print('ROUGE-1 scores:')\n",
    "print(\"Training scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rouge1_good_scores[:int(train_size * len(rouge1_good_scores))]) / len(rouge1_good_scores[:int(train_size * len(rouge1_good_scores))]))\n",
    "print('\\nBad scores mean:', sum(rouge1_bad_scores[:int(train_size * len(rouge1_bad_scores))]) / len(rouge1_bad_scores[:int(train_size * len(rouge1_bad_scores))]))\n",
    "print(\"\\nDPO scores mean:\", sum(rouge1_dpo_scores[:int(train_size * len(rouge1_dpo_scores))]) / len(rouge1_dpo_scores[:int(train_size * len(rouge1_dpo_scores))]))\n",
    "print(\"\\nSFT scores mean:\", sum(rouge1_sft_scores[:int(train_size * len(rouge1_sft_scores))]) / len(rouge1_sft_scores[:int(train_size * len(rouge1_sft_scores))]))\n",
    "print(\"--------------------\")\n",
    "print(\"Test scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rouge1_good_scores[int(train_size * len(rouge1_good_scores)):]) / len(rouge1_good_scores[int(train_size * len(rouge1_good_scores)):]))\n",
    "print('\\nBad scores mean:', sum(rouge1_bad_scores[int(train_size * len(rouge1_bad_scores)):]) / len(rouge1_bad_scores[int(train_size * len(rouge1_bad_scores)):]))\n",
    "print(\"\\nDPO scores mean:\", sum(rouge1_dpo_scores[int(train_size * len(rouge1_dpo_scores)):]) / len(rouge1_dpo_scores[int(train_size * len(rouge1_dpo_scores)):]))\n",
    "print(\"\\nSFT scores mean:\", sum(rouge1_sft_scores[int(train_size * len(rouge1_sft_scores)):]) / len(rouge1_sft_scores[int(train_size * len(rouge1_sft_scores)):]))\n",
    "print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rouge-2 F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-2 scores:\n",
      "Training scores:\n",
      "--------------------\n",
      "Good scores mean: 0.39880191527739184\n",
      "\n",
      "Bad scores mean: 0.3396875945166942\n",
      "\n",
      "DPO scores mean: 0.3011038206105337\n",
      "\n",
      "SFT scores mean: 0.5417822128371445\n",
      "--------------------\n",
      "Test scores:\n",
      "--------------------\n",
      "Good scores mean: 0.3956736961852618\n",
      "\n",
      "Bad scores mean: 0.3438119033575253\n",
      "\n",
      "DPO scores mean: 0.2950888306599134\n",
      "\n",
      "SFT scores mean: 0.5359303784791478\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "rouge2_good_scores = [score['rouge2'][0] for score in good_scores]\n",
    "rouge2_bad_scores = [score['rouge2'][0] for score in bad_scores]\n",
    "rouge2_dpo_scores = [score['rouge2'][0] for score in dpo_scores]\n",
    "rouge2_sft_scores = [score['rouge2'][0] for score in sft_scores]\n",
    "print('ROUGE-2 scores:')\n",
    "print(\"Training scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rouge2_good_scores[:int(train_size * len(rouge2_good_scores))]) / len(rouge2_good_scores[:int(train_size * len(rouge2_good_scores))]))\n",
    "print('\\nBad scores mean:', sum(rouge2_bad_scores[:int(train_size * len(rouge2_bad_scores))]) / len(rouge2_bad_scores[:int(train_size * len(rouge2_bad_scores))]))\n",
    "print(\"\\nDPO scores mean:\", sum(rouge2_dpo_scores[:int(train_size * len(rouge2_dpo_scores))]) / len(rouge2_dpo_scores[:int(train_size * len(rouge2_dpo_scores))]))\n",
    "print(\"\\nSFT scores mean:\", sum(rouge2_sft_scores[:int(train_size * len(rouge2_sft_scores))]) / len(rouge2_sft_scores[:int(train_size * len(rouge2_sft_scores))]))\n",
    "print(\"--------------------\")\n",
    "print(\"Test scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rouge2_good_scores[int(train_size * len(rouge2_good_scores)):]) / len(rouge2_good_scores[int(train_size * len(rouge2_good_scores)):]))\n",
    "print('\\nBad scores mean:', sum(rouge2_bad_scores[int(train_size * len(rouge2_bad_scores)):]) / len(rouge2_bad_scores[int(train_size * len(rouge2_bad_scores)):]))\n",
    "print(\"\\nDPO scores mean:\", sum(rouge2_dpo_scores[int(train_size * len(rouge2_dpo_scores)):]) / len(rouge2_dpo_scores[int(train_size * len(rouge2_dpo_scores)):]))\n",
    "print(\"\\nSFT scores mean:\", sum(rouge2_sft_scores[int(train_size * len(rouge2_sft_scores)):]) / len(rouge2_sft_scores[int(train_size * len(rouge2_sft_scores)):]))\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rouge-L F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-L scores:\n",
      "Training scores:\n",
      "--------------------\n",
      "Good scores mean: 0.5562012254199095\n",
      "\n",
      "Bad scores mean: 0.44100793324710685\n",
      "\n",
      "DPO scores mean: 0.4204164376359979\n",
      "\n",
      "SFT scores mean: 0.6245999721312844\n",
      "--------------------\n",
      "Test scores:\n",
      "--------------------\n",
      "Good scores mean: 0.5546049449790932\n",
      "\n",
      "Bad scores mean: 0.4424617051811035\n",
      "\n",
      "DPO scores mean: 0.41436384195624537\n",
      "\n",
      "SFT scores mean: 0.6191841510223639\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "rougeL_good_scores = [score['rougeL'][0] for score in good_scores]\n",
    "rougeL_bad_scores = [score['rougeL'][0] for score in bad_scores]\n",
    "rougeL_dpo_scores = [score['rougeL'][0] for score in dpo_scores]\n",
    "rougeL_sft_scores = [score['rougeL'][0] for score in sft_scores]\n",
    "print('ROUGE-L scores:')\n",
    "print(\"Training scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rougeL_good_scores[:int(train_size * len(rougeL_good_scores))]) / len(rougeL_good_scores[:int(train_size * len(rougeL_good_scores))]))\n",
    "print('\\nBad scores mean:', sum(rougeL_bad_scores[:int(train_size * len(rougeL_bad_scores))]) / len(rougeL_bad_scores[:int(train_size * len(rougeL_bad_scores))]))\n",
    "print(\"\\nDPO scores mean:\", sum(rougeL_dpo_scores[:int(train_size * len(rougeL_dpo_scores))]) / len(rougeL_dpo_scores[:int(train_size * len(rougeL_dpo_scores))]))\n",
    "print(\"\\nSFT scores mean:\", sum(rougeL_sft_scores[:int(train_size * len(rougeL_sft_scores))]) / len(rougeL_sft_scores[:int(train_size * len(rougeL_sft_scores))]))\n",
    "print(\"--------------------\")\n",
    "print(\"Test scores:\")\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(rougeL_good_scores[int(train_size * len(rougeL_good_scores)):]) / len(rougeL_good_scores[int(train_size * len(rougeL_good_scores)):]))\n",
    "print('\\nBad scores mean:', sum(rougeL_bad_scores[int(train_size * len(rougeL_bad_scores)):]) / len(rougeL_bad_scores[int(train_size * len(rougeL_bad_scores)):]))\n",
    "print(\"\\nDPO scores mean:\", sum(rougeL_dpo_scores[int(train_size * len(rougeL_dpo_scores)):]) / len(rougeL_dpo_scores[int(train_size * len(rougeL_dpo_scores)):]))\n",
    "print(\"\\nSFT scores mean:\", sum(rougeL_sft_scores[int(train_size * len(rougeL_sft_scores)):]) / len(rougeL_sft_scores[int(train_size * len(rougeL_sft_scores)):]))\n",
    "print(\"--------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "my_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
