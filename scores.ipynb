{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar  5 09:07:05 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.04             Driver Version: 570.124.04     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 4000 Ada Gene...    Off |   00000000:01:00.0  On |                  Off |\n",
      "| 30%   43C    P2             33W /  130W |     897MiB /  20475MiB |      9%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A          184961      G   /usr/libexec/Xorg                       119MiB |\n",
      "|    0   N/A  N/A          185076      G   /usr/bin/gnome-shell                     55MiB |\n",
      "|    0   N/A  N/A          186226      G   /usr/lib64/firefox/firefox              152MiB |\n",
      "|    0   N/A  N/A          197432      C   ...ria.abboud/my_venv/bin/python        508MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6498/6498 [31:21<00:00,  3.45it/s]  \n"
     ]
    }
   ],
   "source": [
    "import bert_score\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_bertscore_fast(reference_summary, generated_summary, model_type=\"distilbert-base-uncased\"):\n",
    "    P, R, F1 = bert_score.score([generated_summary], [reference_summary], model_type=model_type)\n",
    "    return {\"Precision\": P.item(), \"Recall\": R.item(), \"F1-score\": F1.item()}\n",
    "\n",
    "articles = json.load(open(\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/summaries/2k_finetuned_dpo_summarized_wikipedia_articles.json\"))\n",
    "\n",
    "scores = []\n",
    "\n",
    "\n",
    "for article in tqdm(articles):\n",
    "    reference_summary = article[\"content\"]\n",
    "    good_summary = article[\"summary\"]\n",
    "\n",
    "    score = compute_bertscore_fast(reference_summary, good_summary)\n",
    "    scores.append(score)\n",
    "\n",
    "json.dump(scores, open(\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/scores/dpo_scores_2k\", \"w\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(scores, open(\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/scores/dpo_scores_2k.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_scores = json.load(open('scores/good_scores.json', 'r'))\n",
    "bad_scores = json.load(open('scores/bad_scores.json', 'r'))\n",
    "dpo_scores = json.load(open('scores/dpo_scores.json', 'r'))\n",
    "sft_scores = json.load(open('scores/SFT_scores.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_good_scores = [score['F1-score'] for score in good_scores]\n",
    "F1_bad_scores = [score['F1-score'] for score in bad_scores]\n",
    "F1_dpo_scores = [score['F1-score'] for score in dpo_scores]\n",
    "\n",
    "Precision_good_scores = [score['Precision'] for score in good_scores]\n",
    "Precision_bad_scores = [score['Precision'] for score in bad_scores]\n",
    "Precision_dpo_scores = [score['Precision'] for score in dpo_scores]\n",
    "\n",
    "Recall_good_scores = [score['Recall'] for score in good_scores]\n",
    "Recall_bad_scores = [score['Recall'] for score in bad_scores]\n",
    "Recall_dpo_scores = [score['Recall'] for score in dpo_scores]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-scores:\n",
      "--------------------\n",
      "Good scores mean: 0.6820049513083707\n",
      "\n",
      "Bad scores mean: 0.6423906561939934\n",
      "\n",
      "DPO scores mean: 0.7699803263281942\n",
      "\n",
      "SFT scores mean: 0.8301981940897621\n",
      "--------------------\n",
      "Precision:\n",
      "--------------------\n",
      "Good scores mean: 0.7453625507224461\n",
      "\n",
      "Bad scores mean: 0.7014457143989976\n",
      "\n",
      "DPO scores mean: 0.8150254339070526\n",
      "\n",
      "SFT scores mean: 0.8795502921385778\n",
      "--------------------\n",
      "Recall:\n",
      "--------------------\n",
      "Good scores mean: 0.6310297233010365\n",
      "\n",
      "Bad scores mean: 0.5946983401547257\n",
      "\n",
      "DPO scores mean: 0.7303245136971738\n",
      "\n",
      "SFT scores mean: 0.7873001057722269\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print('F1-scores:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(F1_good_scores) / len(F1_good_scores))\n",
    "print('\\nBad scores mean:', sum(F1_bad_scores) / len(F1_bad_scores))\n",
    "print(\"\\nDPO scores mean:\", sum(F1_dpo_scores) / len(F1_dpo_scores))\n",
    "print(\"--------------------\")\n",
    "print('Precision:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(Precision_good_scores) / len(Precision_good_scores))\n",
    "print('\\nBad scores mean:', sum(Precision_bad_scores) / len(Precision_bad_scores))\n",
    "print(\"\\nDPO scores mean:\", sum(Precision_dpo_scores) / len(Precision_dpo_scores))\n",
    "print(\"--------------------\")\n",
    "print('Recall:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(Recall_good_scores) / len(Recall_good_scores))\n",
    "print('\\nBad scores mean:', sum(Recall_bad_scores) / len(Recall_bad_scores))\n",
    "print(\"\\nDPO scores mean:\", sum(Recall_dpo_scores) / len(Recall_dpo_scores))\n",
    "print(\"--------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score for 2k tokens documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-scores:\n",
      "--------------------\n",
      "Good scores mean: 0.8144641416221221\n",
      "\n",
      "Bad scores mean: 0.797419783580296\n",
      "\n",
      "SFT scores mean: 0.8301981940897621\n",
      "\n",
      "DPO scores mean: 0.7962014763382407\n",
      "--------------------\n",
      "Precision:\n",
      "--------------------\n",
      "Good scores mean: 0.8533857606665036\n",
      "\n",
      "Bad scores mean: 0.8363323543695795\n",
      "\n",
      "SFT scores mean: 0.8795502921385778\n",
      "\n",
      "DPO scores mean: 0.8336767801764415\n",
      "--------------------\n",
      "Recall:\n",
      "--------------------\n",
      "Good scores mean: 0.7798641234672673\n",
      "\n",
      "Bad scores mean: 0.7628903748005028\n",
      "\n",
      "SFT scores mean: 0.7873001057722269\n",
      "\n",
      "DPO scores mean: 0.7628200178551432\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "good_scores = json.load(open('scores/good_scores_2k.json', 'r'))\n",
    "bad_scores = json.load(open('scores/bad_scores_2k.json', 'r'))\n",
    "sft_scores = json.load(open('scores/SFT_scores.json', 'r'))\n",
    "dpo_scores = json.load(open('scores/dpo_scores_2k.json', 'r'))\n",
    "\n",
    "\n",
    "F1_good_scores = [score['F1-score'] for score in good_scores]\n",
    "F1_bad_scores = [score['F1-score'] for score in bad_scores]\n",
    "F1_sft_scores = [score['F1-score'] for score in sft_scores]\n",
    "F1_dpo_scores = [score['F1-score'] for score in dpo_scores]\n",
    "\n",
    "Precision_good_scores = [score['Precision'] for score in good_scores]\n",
    "Precision_bad_scores = [score['Precision'] for score in bad_scores]\n",
    "Precision_sft_scores = [score['Precision'] for score in sft_scores]\n",
    "Precision_dpo_scores = [score['Precision'] for score in dpo_scores]\n",
    "\n",
    "Recall_good_scores = [score['Recall'] for score in good_scores]\n",
    "Recall_bad_scores = [score['Recall'] for score in bad_scores]\n",
    "Recall_sft_scores = [score['Recall'] for score in sft_scores]\n",
    "Recall_dpo_scores = [score['Recall'] for score in dpo_scores]\n",
    "\n",
    "print('F1-scores:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(F1_good_scores) / len(F1_good_scores))\n",
    "print('\\nBad scores mean:', sum(F1_bad_scores) / len(F1_bad_scores))\n",
    "print(\"\\nSFT scores mean:\", sum(F1_sft_scores) / len(F1_sft_scores))\n",
    "print(\"\\nDPO scores mean:\", sum(F1_dpo_scores) / len(F1_dpo_scores))\n",
    "print(\"--------------------\")\n",
    "print('Precision:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(Precision_good_scores) / len(Precision_good_scores))\n",
    "print('\\nBad scores mean:', sum(Precision_bad_scores) / len(Precision_bad_scores))\n",
    "print(\"\\nSFT scores mean:\", sum(Precision_sft_scores) / len(Precision_sft_scores))\n",
    "print(\"\\nDPO scores mean:\", sum(Precision_dpo_scores) / len(Precision_dpo_scores))\n",
    "print(\"--------------------\")\n",
    "print('Recall:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(Recall_good_scores) / len(Recall_good_scores))\n",
    "print('\\nBad scores mean:', sum(Recall_bad_scores) / len(Recall_bad_scores))\n",
    "print(\"\\nSFT scores mean:\", sum(Recall_sft_scores) / len(Recall_sft_scores))\n",
    "print(\"\\nDPO scores mean:\", sum(Recall_dpo_scores) / len(Recall_dpo_scores))\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposing the scores into training and testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = json.load(open(\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/data/train.json\"))\n",
    "\n",
    "good_scores = json.load(open('scores/good_scores.json', 'r'))\n",
    "bad_scores = json.load(open('scores/bad_scores.json', 'r'))\n",
    "dpo_scores = json.load(open('scores/dpo_scores.json', 'r'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-scores on the training set:\n",
      "--------------------\n",
      "Good scores mean: 0.6822357793154603\n",
      "\n",
      "Bad scores mean: 0.6427095072441382\n",
      "\n",
      "DPO scores mean: 0.7699586713280757\n"
     ]
    }
   ],
   "source": [
    "# Compute the average F1-score for each model on the training set\n",
    "good_scores_train = good_scores[:len(train)]\n",
    "bad_scores_train = bad_scores[:len(train)]\n",
    "dpo_scores_train = dpo_scores[:len(train)]\n",
    "\n",
    "F1_good_scores_train = [score['F1-score'] for score in good_scores_train]\n",
    "F1_bad_scores_train = [score['F1-score'] for score in bad_scores_train]\n",
    "F1_dpo_scores_train = [score['F1-score'] for score in dpo_scores_train]\n",
    "\n",
    "print('F1-scores on the training set:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(F1_good_scores_train) / len(F1_good_scores_train))\n",
    "print('\\nBad scores mean:', sum(F1_bad_scores_train) / len(F1_bad_scores_train))\n",
    "print(\"\\nDPO scores mean:\", sum(F1_dpo_scores_train) / len(F1_dpo_scores_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-scores on the test set:\n",
      "--------------------\n",
      "Good scores mean: 0.6810821079663214\n",
      "\n",
      "Bad scores mean: 0.6411158994067139\n",
      "\n",
      "DPO scores mean: 0.770066902359125\n"
     ]
    }
   ],
   "source": [
    "# Compute the average F1-score for each model on the test set\n",
    "good_scores_test = good_scores[len(train):]\n",
    "bad_scores_test = bad_scores[len(train):]\n",
    "dpo_scores_test = dpo_scores[len(train):]\n",
    "\n",
    "F1_good_scores_test = [score['F1-score'] for score in good_scores_test]\n",
    "F1_bad_scores_test = [score['F1-score'] for score in bad_scores_test]\n",
    "F1_dpo_scores_test = [score['F1-score'] for score in dpo_scores_test]\n",
    "\n",
    "print('F1-scores on the test set:')\n",
    "print(\"--------------------\")\n",
    "print('Good scores mean:', sum(F1_good_scores_test) / len(F1_good_scores_test))\n",
    "print('\\nBad scores mean:', sum(F1_bad_scores_test) / len(F1_bad_scores_test))\n",
    "print(\"\\nDPO scores mean:\", sum(F1_dpo_scores_test) / len(F1_dpo_scores_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "my_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
