{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar  5 12:14:27 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.04             Driver Version: 570.124.04     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 4000 Ada Gene...    Off |   00000000:01:00.0  On |                  Off |\n",
      "| 30%   31C    P2             29W /  130W |    1142MiB /  20475MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A          197432      C   ...ria.abboud/my_venv/bin/python        508MiB |\n",
      "|    0   N/A  N/A          209816      C   ...ria.abboud/my_venv/bin/python        508MiB |\n",
      "|    0   N/A  N/A          267389      G   /usr/libexec/Xorg                        58MiB |\n",
      "|    0   N/A  N/A          267415      G   /usr/bin/gnome-shell                     18MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/zakaria.abboud/.cache/huggingface\n",
      "New cache directory: /Data/zakaria.abboud/.cache/huggingface\n",
      "Path from root: /Data/zakaria.abboud/.cache/huggingface\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cache_dir = os.getenv(\"HF_HOME\", os.path.expanduser(\"~/.cache/huggingface\"))\n",
    "print(cache_dir)\n",
    "os.chdir(\"/\")\n",
    "new_cache_dir = \"/Data/zakaria.abboud/.cache/huggingface\"\n",
    "if not os.path.exists(new_cache_dir):\n",
    "    os.makedirs(new_cache_dir, exist_ok=True)\n",
    "\n",
    "# Set the new cache directory\n",
    "os.environ[\"HF_HOME\"] = new_cache_dir\n",
    "print(f\"New cache directory: {new_cache_dir}\")\n",
    "print(f\"Path from root: {os.path.abspath(new_cache_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model to fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13271040 || all params: 328390528 || trainable%: 4.041237145548851\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules = [\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = PeftModel(model, peft_config=config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = json.load(open(\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/data_2k_tokens/train.json\"))\n",
    "test_dataset = json.load(open(\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/data_2k_tokens/test.json\"))\n",
    "\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "test_dataset = Dataset.from_list(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "\n",
    "    prompt = f\"<human>: Summarize the following text :\\n\\n {data_point['content']}\\n\\nYou should respond in French, with 5 sentences maximum.\\n\\n<assistant>:\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)+tokenizer.eos_token # eos token is important here or the model will not learn how to stop.\n",
    "    tokenized_full_prompt = tokenizer(full_prompt, return_tensors='pt')\n",
    "    ## FILL THE GAP: create the labels first by cloning input_ids\n",
    "    labels = tokenized_full_prompt.input_ids.clone()\n",
    "\n",
    "    prompt = full_prompt[:full_prompt.find(\"<assistant>\")] + \"<assistant>:\"\n",
    "    \n",
    "    assistant_idx = tokenizer.encode(prompt, return_tensors='pt').shape[1]\n",
    "    labels[:, :assistant_idx] = -100\n",
    "\n",
    "    return {\n",
    "        'input_ids': tokenized_full_prompt.input_ids.flatten(),\n",
    "        'labels': labels.flatten(),\n",
    "        'attention_mask': tokenized_full_prompt.attention_mask.flatten(),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data for the DPO fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6810dc194ffe4eb0b65379aae6119ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4589 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fb769792a24197aa9296a2bff5580d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1967 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_data_dpo(data_point):\n",
    "\n",
    "    system = \"You are a helpfull assistant, and you are asked to summarize the following text in French. You should respond with 5 sentences maximum:\\n\\n\"\n",
    "\n",
    "    return {\n",
    "            \"chosen\": [\n",
    "                {'role': 'user', 'content': system +\"\\n\"+ data_point[\"content\"]},\n",
    "                {'role': 'assistant', 'content': data_point[\"chosen\"]}\n",
    "            ],\n",
    "            \"rejected\": [\n",
    "                {'role': 'user', 'content': system +\"\\n\"+ data_point[\"content\"]},\n",
    "                {'role': 'assistant', 'content': data_point[\"rejected\"]}\n",
    "            ],\n",
    "        }\n",
    "\n",
    "data_dpo = train_dataset.shuffle(seed=42).map(preprocess_data_dpo)\n",
    "# val_dpo = val_dataset.shuffle(seed=42).map(preprocess_data_dpo)\n",
    "test_dpo = test_dataset.shuffle(seed=42).map(preprocess_data_dpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['title', 'content', 'chosen', 'rejected'],\n",
      "    num_rows: 4589\n",
      "})\n",
      "{'title': 'Córrego do Bom Jesus (Part 1)', 'content': \"Córrego do Bom Jesus est une municipalité brésilienne de l'État du Minas Gerais et la microrégion de Pouso Alegre.\\n\\n\\n== Notes et références ==\\n\\n Portail du Minas Gerais\", 'chosen': [{'content': \"You are a helpfull assistant, and you are asked to summarize the following text in French. You should respond with 5 sentences maximum:\\n\\n\\nCórrego do Bom Jesus est une municipalité brésilienne de l'État du Minas Gerais et la microrégion de Pouso Alegre.\\n\\n\\n== Notes et références ==\\n\\n Portail du Minas Gerais\", 'role': 'user'}, {'content': \"Córrego do Bom Jesus est une municipalité située dans l'État du Minas Gerais au Brésil. Elle fait partie de la microrégion de Pouso Alegre. Pour plus d'informations, consultez le Portail du Minas Gerais. Cette municipalité est donc un territoire administratif et géographique bien définis dans cette région brésilienne. <sentence-break>\", 'role': 'assistant'}], 'rejected': [{'content': \"You are a helpfull assistant, and you are asked to summarize the following text in French. You should respond with 5 sentences maximum:\\n\\n\\nCórrego do Bom Jesus est une municipalité brésilienne de l'État du Minas Gerais et la microrégion de Pouso Alegre.\\n\\n\\n== Notes et références ==\\n\\n Portail du Minas Gerais\", 'role': 'user'}, {'content': \"Ce terme d'île ou de île en général désigne un petit bateau qui a été déplacé pour y survivre. Dans le cas de l'île Corrêa, elle était principalement utilisée par les peuples indigènes au fil des siècles. L'île est maintenant considérée comme une propriété privée. Elle est située dans l'est du Minas Gerais, à environ 180 km au nord-est de Lisbonne. \\n\\n<assistant>: Córrego do Bom Jesus est une municipalité brésilienne située dans l\", 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "print(data_dpo)\n",
    "print(data_dpo[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: what is the capital of France?\n",
      "<assistant>: Paris<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}<human>: {{ message['content'] }}\\n{% elif message['role'] == 'assistant' %}<assistant>: {{ message['content'] }}{{ eos_token }}{% endif %}{% endfor %}{% if add_generation_prompt %}<assistant>: {% endif %}\"\n",
    "\n",
    "messages_example =  [\n",
    "                      {'role': 'user', 'content': 'what is the capital of France?'},\n",
    "                      {'role': 'assistant', 'content': 'Paris'}\n",
    "                    ]\n",
    "\n",
    "print(tokenizer.apply_chat_template(messages_example, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_253845/2269525552.py:20: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `DPOTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = DPOTrainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35909dd6126f4a13bde214fc258fb527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset:   0%|          | 0/4589 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f090ca1029549e7893da60f163afa29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/4589 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f8ca5d5f434af1bcd4c88f12526bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/4589 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  8/200 00:25 < 13:29, 0.24 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OUTPUT_DIR = \"/Data/zakaria.abboud/dpo_output_2k_tokens\"\n",
    "\n",
    "training_args = DPOConfig(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-5,\n",
    "    bf16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=20,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_steps=200,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    report_to=\"tensorboard\",\n",
    "    beta=0.1,\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=data_dpo,\n",
    "    tokenizer=tokenizer,\n",
    "    # Data collator is not needed for DPOTrainer as it internally manages it\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 896)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "              (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "              (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "              (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Save the model weights\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/models/2k_tokens\")\n",
    "tokenizer.save_pretrained(\"users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/models/2k_tokens\")\n",
    "model.config.save_pretrained(\"users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/models/2k_tokens\")\n",
    "print(\"Model saved successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and Computing the new summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/models/2k_tokens\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/models/2k_tokens\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_document(document):\n",
    "    # Tokenize the input document\n",
    "    prompt = f\"<human>: Summarize the following text :\\n\\n{document}\\n\\nYou should respond in French, with 5 sentences maximum.\\n\\n<assistant>:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    # Decode the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    assistant_index = summary.find(\"<assistant>:\")\n",
    "    if assistant_index != -1:\n",
    "        summary = summary[assistant_index + len(\"<assistant>:\"):].strip()\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = json.load(open(\"/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/summaries/2k_wikipedia_articles.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model before computing the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = articles[0][\"content\"]\n",
    "print(summarize_document(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing articles:   0%|          | 0/6498 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing articles: 100%|██████████| 6498/6498 [3:05:53<00:00,  1.72s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized articles saved to '/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/summaries/2k_finetuned_dpo_summarized_wikipedia_articles.json'\n"
     ]
    }
   ],
   "source": [
    "# File to save the summarized articles\n",
    "# output_file = '/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/summaries/summarized_wikipedia_articles.json'\n",
    "output_file = '/users/eleves-a/2022/zakaria.abboud/Desktop/NLP/NLP Projet/summaries/2k_finetuned_dpo_summarized_wikipedia_articles.json'\n",
    "\n",
    "# Function to save results progressively\n",
    "def save_progress(summarized_articles, output_file):\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            existing_data = json.load(f)\n",
    "    else:\n",
    "        existing_data = []\n",
    "\n",
    "    existing_data.extend(summarized_articles)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Summarize articles in parallel\n",
    "summarized_articles = []\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust max_workers based on your system\n",
    "    futures = {executor.submit(summarize_document, article['content']): article for article in articles}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Summarizing articles\"):\n",
    "        result = future.result()\n",
    "        summarized_articles.append({\n",
    "            'title': futures[future]['title'],\n",
    "            'content': futures[future]['content'],\n",
    "            'summary': result\n",
    "        })\n",
    "\n",
    "        # Save progress every 10 summaries\n",
    "        if len(summarized_articles) % 10 == 0:\n",
    "            save_progress(summarized_articles, output_file)\n",
    "            summarized_articles = []  # Clear the list after saving\n",
    "\n",
    "# Save any remaining summaries\n",
    "if summarized_articles:\n",
    "    save_progress(summarized_articles, output_file)\n",
    "\n",
    "print(f\"Summarized articles saved to '{output_file}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "my_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
