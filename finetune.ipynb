{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the environment and installing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets bitsandbytes peft torch sentencepiece -q\n",
    "%pip install rouge-score -q\n",
    "%pip install blobfile tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the preparation, the dataset looks like this:\n",
    "```json\n",
    "{\n",
    "  \"title\": \"Document title...\",\n",
    "  \"content\": \"Original document text...\",\n",
    "  \"summary\": \"Generated summary...\"\n",
    "}\n",
    "```\n",
    "And it's split into `train`, `validation`, and `test` sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files={\"train\": \"data/\"+\"train.json\", \"val\": \"data/\"+\"val.json\", \"test\": \"data/\"+\"test.json\"})\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Custom Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several models interesting to fine-tune for this task. We will use the `transformers` library to fine-tune a model from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"mT5-Base\": \"google/mt5-base\", # 580M params\n",
    "    \"Qwen2.5-0.5B\": \"Qwen/Qwen2.5-0.5B\",\n",
    "}\n",
    "\n",
    "model_name = models[\"Qwen2.5-0.5B\"]\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "# print(f\"Loaded model: {model_name}\\n\", model)\n",
    "# print(model.config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# print(f\"Loaded Tokenizer: \\n\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the tokenizer to encode the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_schema(doc):\n",
    "    return f\"\"\"<human>: résume {doc}\n",
    "<assistant>:\"\"\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ prompt_schema(doc) for doc in examples[\"content\"] ]\n",
    "    targets = [ doc for doc in examples[\"summary\"] ]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=8192, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=8192, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return {\n",
    "        'input_ids': model_inputs[\"input_ids\"],\n",
    "        'labels': model_inputs[\"labels\"],\n",
    "        'attention_mask': model_inputs[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Lora to fine-tune the model efficiently without having to adjust all the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target_modules\n",
    "target_modules = [\n",
    "    'gate_proj',\n",
    "    'up_proj',\n",
    "    'down_proj',\n",
    "]\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "generation_config.do_sample = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "article = \"\"\"Le Monument-Lefebvre est un lieu historique national du Canada situé à Memramcook, au Nouveau-Brunswick.\\n\\n\\n== Histoire ==\\nLe Monument-Lefebvre est inauguré les 16 et 17 juin 1897 et nommé ainsi en l'honneur du père Camille Lefebvre, fondateur du Collège Saint-Joseph, décédé deux ans auparavant.\\nIl est tout d'abord utilisé comme laboratoire par le Collège Saint-Joseph, tandis que son amphithéâtre permet d'organiser des concerts, des conférences et de nombreuses manifestations diverses. Le lieu et son fondateur ont joué un rôle déterminant dans la renaissance acadienne.\\nAprès la fermeture du Collège en 1972, le Monument-Lefebvre manque de disparaître mais est finalement rénové et déclaré lieu historique national en 1994.\\n\\n\\n== Architecture ==\\nLe Monument-Lefebvre est construit en grès rustiqué de couleur olive. Sa façade symétrique comprend des éléments néoromans. Il abrite un théâtre et des salle de classe.\\n\\n\\n== Notes et références ==\\n\\n\\n== Liens externes ==\\nRessources relatives à l'architecture : Édifices fédéraux patrimoniaux Lieux historiques nationaux Répertoire canadien des lieux patrimoniaux \\n(fr + en) Site officiel\\n\\n Portail de l’Acadie   Portail du Nouveau-Brunswick   Portail des lieux patrimoniaux du Canada\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "<human>: résume {article}\n",
    "<assistant>:\n",
    "\"\"\"\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=20,\n",
    "    output_dir=\"results\",\n",
    "    max_steps=6,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"val\"],\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "article = \"\"\"Le Monument-Lefebvre est un lieu historique national du Canada situé à Memramcook, au Nouveau-Brunswick.\\n\\n\\n== Histoire ==\\nLe Monument-Lefebvre est inauguré les 16 et 17 juin 1897 et nommé ainsi en l'honneur du père Camille Lefebvre, fondateur du Collège Saint-Joseph, décédé deux ans auparavant.\\nIl est tout d'abord utilisé comme laboratoire par le Collège Saint-Joseph, tandis que son amphithéâtre permet d'organiser des concerts, des conférences et de nombreuses manifestations diverses. Le lieu et son fondateur ont joué un rôle déterminant dans la renaissance acadienne.\\nAprès la fermeture du Collège en 1972, le Monument-Lefebvre manque de disparaître mais est finalement rénové et déclaré lieu historique national en 1994.\\n\\n\\n== Architecture ==\\nLe Monument-Lefebvre est construit en grès rustiqué de couleur olive. Sa façade symétrique comprend des éléments néoromans. Il abrite un théâtre et des salle de classe.\\n\\n\\n== Notes et références ==\\n\\n\\n== Liens externes ==\\nRessources relatives à l'architecture : Édifices fédéraux patrimoniaux Lieux historiques nationaux Répertoire canadien des lieux patrimoniaux \\n(fr + en) Site officiel\\n\\n Portail de l’Acadie   Portail du Nouveau-Brunswick   Portail des lieux patrimoniaux du Canada\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "<human>: résume {article}\n",
    "<assistant>:\n",
    "\"\"\"\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.input_ids,\n",
    "        attention_mask=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Save the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return result\n",
    "\n",
    "results = compute_metrics(trainer.predict(tokenized_datasets[\"test\"]))\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Bert Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
